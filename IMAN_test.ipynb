{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh, Dropout, Upsample\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch.autograd import Variable\n",
    "import networkx as nx\n",
    "\n",
    "import os.path as osp\n",
    "import pickle\n",
    "from scipy.linalg import sqrtm\n",
    "import argparse\n",
    "from scipy.stats import wasserstein_distance\n",
    "from torch.distributions import normal, kl\n",
    "\n",
    "\n",
    "import argparse\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE, InnerProductDecoder, ARGVA\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_data_tensor\n",
    "\n",
    "lr_train, lr_test, hr_train = load_data_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([167, 160, 160])\n",
      "torch.Size([167, 268, 268])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "print(lr_train.shape)\n",
    "print(hr_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subjects in simulated data \n",
    "N_SUBJECTS = 50\n",
    "\n",
    "# Number of ROIs in source brain graph for simulated data \n",
    "N_SOURCE_NODES = 160\n",
    "\n",
    "# Number of ROIs in target brain graph for simulated data\n",
    "N_TARGET_NODES = 268\n",
    "\n",
    "# Number of traning epochs\n",
    "N_EPOCHS = 100\n",
    "\n",
    "\n",
    "####** DO NOT MODIFY BELOW **####\n",
    "N_SOURCE_NODES_F =int((N_SOURCE_NODES*(N_SOURCE_NODES-1))/2)\n",
    "N_TARGET_NODES_F =int((N_TARGET_NODES*(N_TARGET_NODES-1))/2)\n",
    "###**************************####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aligner(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Aligner, self).__init__()\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES*N_SOURCE_NODES), ReLU())\n",
    "        self.conv1 = NNConv(N_SOURCE_NODES, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv11 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES), ReLU())\n",
    "        self.conv2 = NNConv(N_SOURCE_NODES, 1, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv22 = BatchNorm(1, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES), ReLU())\n",
    "        self.conv3 = NNConv(1, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv33 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.pos_edge_index, data.edge_attr\n",
    "\n",
    "        x1 = F.sigmoid(self.conv11(self.conv1(x, edge_index, edge_attr)))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "\n",
    "        x2 = F.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\n",
    "        x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "        x3 = torch.cat([F.sigmoid(self.conv33(self.conv3(x2, edge_index, edge_attr))), x1], dim=1)\n",
    "        x4 = x3[:, 0:N_SOURCE_NODES]\n",
    "        x5 = x3[:, N_SOURCE_NODES:2*N_SOURCE_NODES]\n",
    "\n",
    "        x6 = (x4 + x5) / 2\n",
    "        return x6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES*N_SOURCE_NODES),ReLU())\n",
    "        self.conv1 = NNConv(N_SOURCE_NODES, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv11 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_TARGET_NODES*N_SOURCE_NODES), ReLU())\n",
    "        self.conv2 = NNConv(N_TARGET_NODES, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv22 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_TARGET_NODES*N_SOURCE_NODES), ReLU())\n",
    "        self.conv3 = NNConv(N_SOURCE_NODES, N_TARGET_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv33 = BatchNorm(N_TARGET_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "\n",
    "        # self.layer= torch.nn.ConvTranspose2d(N_TARGET_NODES, N_TARGET_NODES,5)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.pos_edge_index, data.edge_attr\n",
    "        # x = torch.squeeze(x)\n",
    "\n",
    "        x1 = F.sigmoid(self.conv11(self.conv1(x, edge_index, edge_attr)))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "\n",
    "        # x2 = F.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\n",
    "        # x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "        x3 = F.sigmoid(self.conv33(self.conv3(x1, edge_index, edge_attr)))\n",
    "        x3 = F.dropout(x3, training=self.training)\n",
    "\n",
    "\n",
    "\n",
    "        x4  = torch.matmul(x3.t(), x3)\n",
    "\n",
    "        return x4\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = GCNConv(N_TARGET_NODES, N_TARGET_NODES, cached=True)\n",
    "        self.conv2 = GCNConv(N_TARGET_NODES, 1, cached=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.pos_edge_index, data.edge_attr\n",
    "        x = torch.squeeze(x)\n",
    "        x1 = F.sigmoid(self.conv1(x, edge_index))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "        x2 = F.sigmoid(self.conv2(x1, edge_index))\n",
    "        #         # x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back into a 2D symmetric array\n",
    "\n",
    "\n",
    "def topological_measures(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology = []\n",
    "\n",
    "\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_matrix(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "    # compute closeness centrality and transform the output to vector\n",
    "    cc = nx.closeness_centrality(U, distance=\"weight\")\n",
    "    closeness_centrality = np.array([cc[g] for g in U])\n",
    "    # compute betweeness centrality and transform the output to vector\n",
    "    # bc = nx.betweenness_centrality(U, weight='weight')\n",
    "    # bc = (nx.betweenness_centrality(U))\n",
    "    betweenness_centrality = np.array([cc[g] for g in U])\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "    topology.append(closeness_centrality)  # 0\n",
    "    topology.append(betweenness_centrality)  # 1\n",
    "    topology.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology\n",
    "# put it back into a 2D symmetric array\n",
    "\n",
    "def eigen_centrality(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology_eigen = []\n",
    "\n",
    "\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_matrix(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "\n",
    "    topology_eigen.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology_eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on CPU\")\n",
    "\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "adversarial_loss.to(device)\n",
    "l1_loss.to(device)\n",
    "\n",
    "\n",
    "def pearson_coor(input, target):\n",
    "    vx = input - torch.mean(input)\n",
    "    vy = target - torch.mean(target)\n",
    "    cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def GT_loss(target, predicted):\n",
    "\n",
    "    # l1_loss\n",
    "    loss_pix2pix = l1_loss(target, predicted)\n",
    "\n",
    "    # topological_loss\n",
    "    target_n = target.detach().cpu().clone().numpy()\n",
    "    predicted_n = predicted.detach().cpu().clone().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    target_t = eigen_centrality(target_n)\n",
    "    real_topology = torch.tensor(target_t)\n",
    "    predicted_t = eigen_centrality(predicted_n)\n",
    "    fake_topology = torch.tensor(predicted_t)\n",
    "    topo_loss = l1_loss(fake_topology, real_topology)\n",
    "\n",
    "    pc_loss = pearson_coor(target, predicted).to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    G_loss = loss_pix2pix + (1 - pc_loss) + topo_loss\n",
    "\n",
    "    return G_loss\n",
    "\n",
    "\n",
    "def Alignment_loss(target, predicted):\n",
    "    # l_loss1 = torch.abs(nn.KLDivLoss()(F.softmax(zt1), F.softmax(z_s1.t())))\n",
    "\n",
    "    kl_loss = torch.abs(F.kl_div(F.softmax(target), F.softmax(predicted), None, None, 'sum'))\n",
    "    kl_loss = (1/350) * kl_loss\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "#  GAN\n",
    "aligner = Aligner()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "# Losses\n",
    "adversarial_loss1 = torch.nn.BCELoss()\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "# send 1st GAN to GPU\n",
    "aligner.to(device)\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "adversarial_loss1.to(device)\n",
    "l1_loss.to(device)\n",
    "\n",
    "Aligner_optimizer = torch.optim.AdamW(aligner.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "def IMANGraphNet (X_train_source, X_test_source, X_train_target, X_test_target):\n",
    "\n",
    "    X_casted_train_source = cast_data_vector_RH(X_train_source)\n",
    "    X_casted_test_source = cast_data_vector_RH(X_test_source)\n",
    "    X_casted_train_target = cast_data_vector_FC(X_train_target)\n",
    "    X_casted_test_target = cast_data_vector_FC(X_test_target)\n",
    "\n",
    "    aligner.train()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    nbre_epochs = N_EPOCHS\n",
    "    for epochs in range(nbre_epochs):\n",
    "        # Train Generator\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            Al_losses = []\n",
    "\n",
    "\n",
    "            Ge_losses = []\n",
    "            losses_discriminator = []\n",
    "\n",
    "            i = 0\n",
    "            for data_source, data_target in zip(X_casted_train_source, X_casted_train_target):\n",
    "                # print(i)\n",
    "                targett = data_target.edge_attr.view(N_TARGET_NODES, N_TARGET_NODES)\n",
    "                # ************    Domain alignment    ************\n",
    "                A_output = aligner(data_source)\n",
    "                A_casted = convert_generated_to_graph_Al(A_output)\n",
    "                A_casted = A_casted[0]\n",
    "\n",
    "                target = data_target.edge_attr.view(N_TARGET_NODES, N_TARGET_NODES).detach().cpu().clone().numpy()\n",
    "                target_mean = np.mean(target)\n",
    "                target_std = np.std(target)\n",
    "\n",
    "                d_target = torch.normal(target_mean, target_std, size=(1, N_SOURCE_NODES_F))\n",
    "                dd_target = cast_data_vector_RH(d_target)\n",
    "                dd_target = dd_target[0]\n",
    "                target_d = dd_target.edge_attr.view(N_SOURCE_NODES, N_SOURCE_NODES)\n",
    "\n",
    "                kl_loss = Alignment_loss(target_d, A_output)\n",
    "\n",
    "                Al_losses.append(kl_loss)\n",
    "\n",
    "                # ************     Super-resolution    ************\n",
    "                G_output = generator(A_casted)  # 35 x 35\n",
    "                # print(\"G_output: \", G_output.shape)\n",
    "                G_output_reshaped = (G_output.view(1, N_TARGET_NODES, N_TARGET_NODES, 1).type(torch.FloatTensor)).detach()\n",
    "                G_output_casted = convert_generated_to_graph(G_output_reshaped)\n",
    "                G_output_casted = G_output_casted[0]\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                Gg_loss = GT_loss(targett, G_output)\n",
    "                torch.cuda.empty_cache()\n",
    "                D_real = discriminator(data_target)\n",
    "                D_fake = discriminator(G_output_casted)\n",
    "                torch.cuda.empty_cache()\n",
    "                G_adversarial = adversarial_loss(D_fake, (torch.ones_like(D_fake, requires_grad=False)))\n",
    "                G_loss = G_adversarial + Gg_loss\n",
    "                Ge_losses.append(G_loss)\n",
    "\n",
    "                D_real_loss = adversarial_loss(D_real, (torch.ones_like(D_real, requires_grad=False)))\n",
    "                # torch.cuda.empty_cache()\n",
    "                D_fake_loss = adversarial_loss(D_fake.detach(), torch.zeros_like(D_fake))\n",
    "                D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "                # torch.cuda.empty_cache()\n",
    "                losses_discriminator.append(D_loss)\n",
    "                i += 1\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "            Ge_losses = torch.mean(torch.stack(Ge_losses))\n",
    "            Ge_losses.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            Aligner_optimizer.zero_grad()\n",
    "            Al_losses = torch.mean(torch.stack(Al_losses))\n",
    "            Al_losses.backward(retain_graph=True)\n",
    "            Aligner_optimizer.step()\n",
    "\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            losses_discriminator = torch.mean(torch.stack(losses_discriminator))\n",
    "            losses_discriminator.backward(retain_graph=True)\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        print(\"[Epoch: %d]| [Al loss: %f]| [Ge loss: %f]| [D loss: %f]\" % (epochs, Al_losses, Ge_losses, losses_discriminator))\n",
    "\n",
    "    torch.save(aligner.state_dict(), \"./weight\" + \"aligner_fold\" + \"_\" + \".model\")\n",
    "    torch.save(generator.state_dict(), \"./weight\" + \"generator_fold\" + \"_\" + \".model\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # #     ######################################### TESTING PART #########################################\n",
    "    restore_aligner = \"./weight\" + \"aligner_fold\" + \"_\" + \".model\"\n",
    "    restore_generator = \"./weight\" + \"generator_fold\" + \"_\" + \".model\"\n",
    "\n",
    "    aligner.load_state_dict(torch.load(restore_aligner))\n",
    "    generator.load_state_dict(torch.load(restore_generator))\n",
    "\n",
    "    aligner.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    i = 0\n",
    "    predicted_test_graphs = []\n",
    "    losses_test = []\n",
    "    eigenvector_losses_test = []\n",
    "    l1_tests = []\n",
    "    Closeness_test = []\n",
    "    Eigenvector_test = []\n",
    "    for data_source, data_target in zip(X_casted_test_source, X_casted_test_target):\n",
    "        # print(i)\n",
    "        data_source_test = data_source.x.view(N_SOURCE_NODES, N_SOURCE_NODES)\n",
    "        data_target_test = data_target.x.view(N_TARGET_NODES, N_TARGET_NODES)\n",
    "\n",
    "\n",
    "        A_test = aligner(data_source)\n",
    "        A_test_casted = convert_generated_to_graph_Al(A_test)\n",
    "        A_test_casted = A_test_casted[0]\n",
    "        data_target = data_target_test.detach().cpu().clone().numpy()\n",
    "        # ************     Super-resolution    ************\n",
    "        G_output_test = generator(A_test_casted)  # 35 x35\n",
    "        G_output_test_casted = convert_generated_to_graph(G_output_test)\n",
    "        G_output_test_casted = G_output_test_casted[0]\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        L1_test = l1_loss(data_target_test, G_output_test)\n",
    "        # fold= 1\n",
    "        target_test = data_target_test.detach().cpu().clone().numpy()\n",
    "        predicted_test = G_output_test.detach().cpu().clone().numpy()\n",
    "        source_test = data_source_test.detach().cpu().clone().numpy()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        fake_topology_test = torch.tensor(topological_measures(predicted_test))\n",
    "        real_topology_test = torch.tensor(topological_measures(target_test))\n",
    "\n",
    "        eigenvector_test = (l1_loss(fake_topology_test[2], real_topology_test[2]))\n",
    "\n",
    "\n",
    "        l1_tests.append(L1_test.detach().cpu().numpy())\n",
    "        Eigenvector_test.append(eigenvector_test.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "    mean_l1 = np.mean(l1_tests)\n",
    "    mean_eigenvector = np.mean(Eigenvector_test)\n",
    "\n",
    "    # print(\"Mean L1 Loss Test: \", fold_mean_l1_loss)\n",
    "    # print()\n",
    "\n",
    "    losses_test.append(mean_l1)\n",
    "    eigenvector_losses_test.append(mean_eigenvector)\n",
    "\n",
    "    # fold += 1\n",
    "    return (predicted_test, data_target, source_test, losses_test, eigenvector_losses_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\"\"\"#Training\"\"\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on CPU\")\n",
    "\n",
    "source_data = np.random.normal(0, 0.5, (N_SUBJECTS, N_SOURCE_NODES_F))\n",
    "target_data = np.random.normal(0, 0.5, (N_SUBJECTS, N_TARGET_NODES_F))\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1773)\n",
    "\n",
    "fold = 0\n",
    "losses_test = []\n",
    "closeness_losses_test = []\n",
    "# betweenness_losses_test = []\n",
    "eigenvector_losses_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(source_data):\n",
    "    # print( * \"#\" + \" FOLD \" + str(fold) + \" \" +  * \"#\")\n",
    "    X_train_source, X_test_source, X_train_target, X_test_target = source_data[train_index], source_data[test_index], target_data[train_index], target_data[test_index]\n",
    "\n",
    "    predicted_test, data_target, source_test, l1_test, eigenvector_test = IMANGraphNet(X_train_source, X_test_source, X_train_target, X_test_target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_mean = np.mean(l1_test)\n",
    "Eigenvector_test_mean = np.mean(eigenvector_test)\n",
    "plot_source(source_test)\n",
    "plot_target(data_target)\n",
    "plot_target(predicted_test)\n",
    "\n",
    "print(\"Mean L1 Test\", test_mean)\n",
    "\n",
    "print(\"Mean Eigenvector Test\", Eigenvector_test_mean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
