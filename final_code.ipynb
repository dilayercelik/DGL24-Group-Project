{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch_geometric.utils\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from karateclub import Graph2Vec\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "N_TRAIN = 167\n",
    "N_TEST = 112\n",
    "N_LR_NODES = 160\n",
    "N_HR_NODES = 268\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Class for loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixVectorizer:\n",
    "    \"\"\"\n",
    "    A class for transforming between matrices and vector representations.\n",
    "    \n",
    "    This class provides methods to convert a symmetric matrix into a vector (vectorize)\n",
    "    and to reconstruct the matrix from its vector form (anti_vectorize), focusing on \n",
    "    vertical (column-based) traversal and handling of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the MatrixVectorizer instance.\n",
    "        \n",
    "        The constructor currently does not perform any actions but is included for \n",
    "        potential future extensions where initialization parameters might be required.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorize(matrix, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Converts a matrix into a vector by vertically extracting elements.\n",
    "        \n",
    "        This method traverses the matrix column by column, collecting elements from the\n",
    "        upper triangle, and optionally includes the diagonal elements immediately below\n",
    "        the main diagonal based on the include_diagonal flag.\n",
    "        \n",
    "        Parameters:\n",
    "        - matrix (numpy.ndarray): The matrix to be vectorized.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the vectorization.\n",
    "          Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: The vectorized form of the matrix.\n",
    "        \"\"\"\n",
    "        # Determine the size of the matrix based on its first dimension\n",
    "        matrix_size = matrix.shape[0]\n",
    "\n",
    "        # Initialize an empty list to accumulate vector elements\n",
    "        vector_elements = []\n",
    "\n",
    "        # Iterate over columns and then rows to collect the relevant elements\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Collect upper triangle elements\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally include the diagonal elements immediately below the diagonal\n",
    "                        vector_elements.append(matrix[row, col])\n",
    "\n",
    "        return np.array(vector_elements)\n",
    "\n",
    "    @staticmethod\n",
    "    def anti_vectorize(vector, matrix_size, include_diagonal=False):\n",
    "        \"\"\"\n",
    "        Reconstructs a matrix from its vector form, filling it vertically.\n",
    "        \n",
    "        The method fills the matrix by reflecting vector elements into the upper triangle\n",
    "        and optionally including the diagonal elements based on the include_diagonal flag.\n",
    "        \n",
    "        Parameters:\n",
    "        - vector (numpy.ndarray): The vector to be transformed into a matrix.\n",
    "        - matrix_size (int): The size of the square matrix to be reconstructed.\n",
    "        - include_diagonal (bool, optional): Flag to include diagonal elements in the reconstruction.\n",
    "          Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "        - numpy.ndarray: The reconstructed square matrix.\n",
    "        \"\"\"\n",
    "        # Initialize a square matrix of zeros with the specified size\n",
    "        matrix = np.zeros((matrix_size, matrix_size))\n",
    "\n",
    "        # Index to keep track of the current position in the vector\n",
    "        vector_idx = 0\n",
    "\n",
    "        # Fill the matrix by iterating over columns and then rows\n",
    "        for col in range(matrix_size):\n",
    "            for row in range(matrix_size):\n",
    "                # Skip diagonal elements if not including them\n",
    "                if row != col:  \n",
    "                    if row < col:\n",
    "                        # Reflect vector elements into the upper triangle and its mirror in the lower triangle\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "                    elif include_diagonal and row == col + 1:\n",
    "                        # Optionally fill the diagonal elements after completing each column\n",
    "                        matrix[row, col] = vector[vector_idx]\n",
    "                        matrix[col, row] = vector[vector_idx]\n",
    "                        vector_idx += 1\n",
    "\n",
    "        return matrix\n",
    "\n",
    "\n",
    "def multi_anti_vectorize(arr, vectorizer, matrix_size): \n",
    "    return np.array([vectorizer.anti_vectorize(v, matrix_size) for v in arr])\n",
    "\n",
    "# added an input so that the function can be adjusted to everyone's path to the dataset\n",
    "def load_data_tensor(path_to_datasets):\n",
    "    # import data from .csv file\n",
    "    hr_train_raw = pd.read_csv(path_to_datasets + '/hr_train.csv')\n",
    "    lr_train_raw = pd.read_csv(path_to_datasets + '/lr_train.csv')\n",
    "    lr_test_raw = pd.read_csv(path_to_datasets + '/lr_test.csv')\n",
    "\n",
    "    # anti-vectorize \n",
    "    lr_n = 160\n",
    "    hr_n = 268\n",
    "    vectorizer = MatrixVectorizer()\n",
    "    hr_train = multi_anti_vectorize(hr_train_raw.values, vectorizer, hr_n)\n",
    "    lr_train = multi_anti_vectorize(lr_train_raw.values, vectorizer, lr_n)\n",
    "    lr_test = multi_anti_vectorize(lr_test_raw.values, vectorizer, lr_n)\n",
    "\n",
    "    # NOTE the order of return is low res train, low res test, high res train\n",
    "    return torch.Tensor(lr_train), torch.Tensor(lr_test), torch.Tensor(hr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_hop(adj, hop):\n",
    "    adj_new = adj\n",
    "    for i in range(hop-1):\n",
    "        adj_new = torch.bmm(adj_new, adj)\n",
    "    stack_adj = adj_new.reshape(-1,adj.shape[1])\n",
    "    return stack_adj \n",
    "\n",
    "\n",
    "def upsampled_data(tensor, repeats, idxs):\n",
    "    additional_samples = tensor[idxs, :, :]\n",
    "    upsampled_tensor = torch.repeat_interleave(additional_samples, repeats=repeats, dim=0)\n",
    "    return torch.cat([tensor, upsampled_tensor], dim=0)\n",
    "\n",
    "def standardization(matrix, avg=None, std=None):\n",
    "    if avg is None and std is None:\n",
    "        avg = torch.mean(matrix)\n",
    "        std = torch.std(matrix)\n",
    "        return (matrix - avg)/std, avg, std\n",
    "    else:\n",
    "        return (matrix - avg)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for helping define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_steps(num_steps, low=N_LR_NODES, high=N_HR_NODES):\n",
    "    step_size = (high - low) / (num_steps - 1)\n",
    "    steps_list = [round(low + step_size * i) for i in range(num_steps)]\n",
    "    return steps_list\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histogram(test_pred_tensor, target=None):\n",
    "    flattened_tensor = test_pred_tensor.flatten() \n",
    "    # Create the histogram for predictions in purple\n",
    "    plt.hist(flattened_tensor, alpha=0.5, bins=50, density=True, color='purple', label='Prediction')  \n",
    "    if target is not None:\n",
    "        flattened_target = target.flatten()\n",
    "        # Create the histogram for ground truth in yellow\n",
    "        plt.hist(flattened_target, bins=50, alpha=0.5, color='orange', label='Ground Truth', density=True)\n",
    "    plt.title('Histogram of Tensor Values')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend() # This will show the labels in the plot\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def generate_heatmap(single_tensor, save_path=None):\n",
    "    \"\"\"\n",
    "    Produces heatmap of single tensor\n",
    "\n",
    "    input:\n",
    "    - sinlge_tensor: torch.tensor (single adjacency matrix)\n",
    "    - save_path: string (optional) (string with file name)\n",
    "    \"\"\"\n",
    "    assert len(single_tensor.shape) == 2, 'tensor dimensionality is greater than 2 - only pass one matrix'\n",
    "    assert single_tensor.shape[0] == single_tensor.shape[1], 'tensor not square'\n",
    "    plt.imshow(single_tensor, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(tensor_pred, tensor_true):\n",
    "\n",
    "    \"\"\" \n",
    "    tensor_pred and tensor_true should both be tensors of shape\n",
    "    (num_val_samples, hr_dim, hr_dim).\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize lists to store MAEs for each centrality measure\n",
    "    mae_bc = []\n",
    "    mae_ec = []\n",
    "    mae_pc = []\n",
    "\n",
    "    pred_1d_list = []\n",
    "    gt_1d_list = []\n",
    "\n",
    "    # Iterate over each test sample\n",
    "    for i in tqdm(range(len(tensor_pred)), desc='Evaluating Predictions (Can be Slow)'):\n",
    "\n",
    "        pred_matrix = tensor_pred[i].cpu().detach().numpy()\n",
    "        true_matrix = tensor_true[i].cpu().detach().numpy()\n",
    "\n",
    "        # Convert adjacency matrices to NetworkX graphs\n",
    "        pred_graph = nx.from_numpy_array(pred_matrix, edge_attr=\"weight\")\n",
    "        gt_graph = nx.from_numpy_array(true_matrix, edge_attr=\"weight\")\n",
    "\n",
    "        # Compute centrality measures\n",
    "        pred_bc = nx.betweenness_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_ec = nx.eigenvector_centrality(pred_graph, weight=\"weight\")\n",
    "        pred_pc = nx.pagerank(pred_graph, weight=\"weight\")\n",
    "\n",
    "        gt_bc = nx.betweenness_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_ec = nx.eigenvector_centrality(gt_graph, weight=\"weight\")\n",
    "        gt_pc = nx.pagerank(gt_graph, weight=\"weight\")\n",
    "\n",
    "        # Convert centrality dictionaries to lists\n",
    "        pred_bc_values = list(pred_bc.values())\n",
    "        pred_ec_values = list(pred_ec.values())\n",
    "        pred_pc_values = list(pred_pc.values())\n",
    "\n",
    "        gt_bc_values = list(gt_bc.values())\n",
    "        gt_ec_values = list(gt_ec.values())\n",
    "        gt_pc_values = list(gt_pc.values())\n",
    "\n",
    "        # Compute MAEs\n",
    "        mae_bc.append(mean_absolute_error(pred_bc_values, gt_bc_values))\n",
    "        mae_ec.append(mean_absolute_error(pred_ec_values, gt_ec_values))\n",
    "        mae_pc.append(mean_absolute_error(pred_pc_values, gt_pc_values))\n",
    "\n",
    "        # Vectorize matrices\n",
    "        pred_1d_list.append(MatrixVectorizer.vectorize(pred_matrix))\n",
    "        gt_1d_list.append(MatrixVectorizer.vectorize(true_matrix))\n",
    "\n",
    "    # Compute average MAEs\n",
    "    avg_mae_bc = sum(mae_bc) / len(mae_bc)\n",
    "    avg_mae_ec = sum(mae_ec) / len(mae_ec)\n",
    "    avg_mae_pc = sum(mae_pc) / len(mae_pc)\n",
    "\n",
    "    # Concatenate flattened matrices\n",
    "    pred_1d = np.concatenate(pred_1d_list)\n",
    "    gt_1d = np.concatenate(gt_1d_list)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(pred_1d, gt_1d)\n",
    "    pcc = pearsonr(pred_1d, gt_1d)[0]\n",
    "    js_dis = jensenshannon(pred_1d, gt_1d)\n",
    "\n",
    "    print(\"MAE: \", mae)\n",
    "    print(\"PCC: \", pcc)\n",
    "    print(\"Jensen-Shannon Distance: \", js_dis)\n",
    "    print(\"Average MAE betweenness centrality:\", avg_mae_bc)\n",
    "    print(\"Average MAE eigenvector centrality:\", avg_mae_ec)\n",
    "    print(\"Average MAE PageRank centrality:\", avg_mae_pc)\n",
    "    \n",
    "    all_metrics = {\n",
    "        'mae': mae,\n",
    "        'pcc': pcc,\n",
    "        'js_dis': js_dis,\n",
    "        'avg_mae_bc': avg_mae_bc,\n",
    "        'avg_mae_ec': avg_mae_ec,\n",
    "        'avg_mae_pc': avg_mae_pc,\n",
    "    }\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train, lr_test, hr_train = load_data_tensor(\"dgl-icl\") # load in data from designated file path\n",
    "train_idx, val_idx = train_test_split(list(range(len(lr_train))), test_size=0.2, shuffle=True, random_state=0) # split data into training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning initial node embeddings from adjacency matrix using VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for VAE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 3 hop adjacency matrices\n",
    "lr_train3 = adj_hop(lr_train, 3)\n",
    "hr_train3 = adj_hop(hr_train, 3)\n",
    "lr_test3 = adj_hop(lr_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the matrices so they are by rows\n",
    "lr_stack = lr_train.reshape(-1, N_LR_NODES)\n",
    "hr_stack = hr_train.reshape(-1, N_HR_NODES)\n",
    "lr_test = lr_test.reshape(-1, N_LR_NODES)\n",
    "\n",
    "lr_stack3 = lr_train3.reshape(-1, N_LR_NODES)\n",
    "hr_stack3 = hr_train3.reshape(-1, N_HR_NODES)\n",
    "lr_test3 = lr_test3.reshape(-1, N_LR_NODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the values\n",
    "lr_stack_norm, lr_avg, lr_std = standardization(lr_stack)\n",
    "hr_stack_norm, temp1, temp2  = standardization(hr_stack)\n",
    "lr_test_stack_norm = standardization(lr_test, lr_avg, lr_std)\n",
    "\n",
    "lr_stack3_norm, lr_avg3, lr_std3 = standardization(lr_stack3)\n",
    "hr_stack3_norm, temp1, temp2 = standardization(hr_stack3)\n",
    "lr_test_stack3_norm = standardization(lr_test3, lr_avg3, lr_std3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VAE Models for both low and high resolution adjanceny matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(N_LR_NODES, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        self.fc_m = nn.Linear(64,32)\n",
    "        self.fc_std = nn.Linear(64,32)\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(32,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, N_LR_NODES)\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc_m(h1), self.fc_std(h1)\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu,logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "\n",
    " \n",
    "class VAEHR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAEHR, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(N_HR_NODES, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        self.fc_m = nn.Linear(64,32)\n",
    "        self.fc_std = nn.Linear(64,32)\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(32,64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, N_HR_NODES)\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc_m(h1), self.fc_std(h1)\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu,logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def train(model, adj_matrix, num_epoch=200, lr=0.0001, batch_size=64):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for step in range(num_epoch):\n",
    "        n_completed = 0\n",
    "        while n_completed < len(adj_matrix):\n",
    "            optimizer.zero_grad()\n",
    "            batch = adj_matrix[n_completed: n_completed+batch_size]\n",
    "            e, mu, logvar = model(batch)\n",
    "            loss = vae_loss(e,batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            print(loss)\n",
    "            optimizer.step()\n",
    "            n_completed += batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for the low resolution 1-hop adjacency matrix\n",
    "lr_vae = VAELR()\n",
    "train(lr_vae, lr_stack_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for the high resolution 1-hop adjacency matrix\n",
    "hr_vae = VAEHR()\n",
    "train(hr_vae, hr_stack_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for the low resolution 3-hop adjacency matrix\n",
    "lr_vae3 = VAELR()\n",
    "train(lr_vae, lr_stack3_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for the low resolution 3-hop adjacency matrix\n",
    "hr_vae3 = VAEHR()\n",
    "train(hr_vae3, hr_stack3_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training embeddings from VAE based on low resolution 1-hop adjacency matrix\n",
    "lr_vae.eval()\n",
    "mu, logvar = lr_vae.encode(lr_stack_norm)\n",
    "lr_X_dim1 = lr_vae.reparametrize(mu,logvar).reshape(N_TRAIN, N_LR_NODES, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training embeddings from VAE based on high resolution 1-hop adjacency matrix\n",
    "hr_vae.eval()\n",
    "mu, logvar = hr_vae.encode(hr_stack_norm)\n",
    "hr_X_dim1 = hr_vae.reparametrize(mu,logvar).reshape(N_TRAIN, N_HR_NODES, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training embeddings from VAE based on low resolution 3-hop adjacency matrix\n",
    "lr_vae3.eval()\n",
    "mu, logvar = lr_vae3.encode(lr_stack3_norm)\n",
    "lr_X_dim3 = lr_vae3.reparametrize(mu,logvar).reshape(N_TRAIN, N_LR_NODES, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training embeddings from VAE based on high resolution 3-hop adjacency matrix\n",
    "hr_vae3.eval()\n",
    "mu, logvar = hr_vae3.encode(hr_stack3_norm)\n",
    "hr_X_dim3 = hr_vae3.reparametrize(mu,logvar).reshape(N_TRAIN, N_HR_NODES, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testing embeddings from VAE based on low resolution 1-hop adjacency matrix\n",
    "lr_vae.eval()\n",
    "mu, logvar = lr_vae.encode(lr_test_stack_norm)\n",
    "lr_X_dim1_test = lr_vae.reparametrize(mu,logvar).reshape(N_TEST, N_LR_NODES, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testing embeddings from VAE based on low resolution 3-hop adjacency matrix\n",
    "lr_vae3.eval()\n",
    "mu, logvar = lr_vae3.encode(lr_test_stack3_norm)\n",
    "lr_X_dim3_test = lr_vae3.reparametrize(mu,logvar).reshape(N_TEST, N_LR_NODES, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling based on minority class from the HR adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_idx(hr_train):\n",
    "    hr_adj_list = [tensor.numpy() for tensor in hr_train]\n",
    "    graphs = [nx.from_numpy_matrix(adj_matrix, create_using=nx.DiGraph) for adj_matrix in hr_adj_list]\n",
    "\n",
    "    # generate embeddings for each graph\n",
    "    graph2vec = Graph2Vec(dimensions=130)\n",
    "    graph2vec.fit(graphs)\n",
    "    graph_embeddings = graph2vec.get_embedding()\n",
    "\n",
    "    # apply spectral clustering on the embeddings\n",
    "    num_clusters = 2   # we only partition into majority and minority class\n",
    "    clustering_model = SpectralClustering(n_clusters=num_clusters, assign_labels=\"discretize\", random_state=0)\n",
    "    clusters = clustering_model.fit_predict(graph_embeddings)\n",
    "\n",
    "    return list(np.where(clusters == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = upsampling_idx(hr_train[train_idx])\n",
    "repeats = 4 # add repeat the sample for 4 times each\n",
    "\n",
    "# upsample the data\n",
    "lr_train_up = upsampled_data(lr_train[train_idx], repeats, idxs)\n",
    "hr_train_up = upsampled_data(hr_train[train_idx], repeats, idxs)\n",
    "lr_X_dim1_train_up = upsampled_data(lr_X_dim1[train_idx], repeats, idxs)\n",
    "hr_X_dim1_train_up = upsampled_data(hr_X_dim1[train_idx], repeats, idxs)\n",
    "lr_X_dim3_train_up = upsampled_data(lr_X_dim3[train_idx], repeats, idxs)\n",
    "hr_X_dim3_train_up = upsampled_data(hr_X_dim3[train_idx], repeats, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data into a dataloader for training\n",
    "trainloader = DataLoader(list(zip(lr_X_dim1_train_up, lr_X_dim3_train_up, lr_train_up, hr_X_dim1_train_up, hr_X_dim3_train_up, hr_train_up)), shuffle=True, batch_size=32)\n",
    "valloader = DataLoader(list(zip(lr_X_dim1[val_idx], lr_X_dim3[val_idx], lr_train[val_idx], hr_X_dim1[val_idx], hr_X_dim3[val_idx], hr_train[val_idx])), shuffle=True, batch_size=32)\n",
    "testloader = DataLoader(list(zip(lr_X_dim1_test, lr_X_dim3_test, lr_test)), shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define layers used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define module that stacks GCN layeres\n",
    "class StackedGCN(nn.Module):\n",
    "    def __init__(self, n_nodes, channel_ls, dropout):\n",
    "        super().__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.gcn_layers, self.batch_norm_layers = self._init_layers(channel_ls)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        for i in range(len(self.gcn_layers)):\n",
    "            gcn = self.gcn_layers[i]\n",
    "            batch_norm = self.batch_norm_layers[i]\n",
    "            graph_batch = self._create_batch(X, A)\n",
    "            \n",
    "            X = F.sigmoid(gcn(graph_batch.x, graph_batch.edge_index, graph_batch.edge_attr).reshape(*X.shape[:2], -1))\n",
    "            torch.cuda.empty_cache()\n",
    "            X = F.dropout(batch_norm(X), self.dropout, training=self.training)\n",
    "            torch.cuda.empty_cache()\n",
    "        return X\n",
    "\n",
    "    # initialise stacks of GCN layer -> batch normalisation\n",
    "    def _init_layers(self, channel_ls):\n",
    "        layers_ls = []\n",
    "        batch_norm_ls = []\n",
    "        for i in range(len(channel_ls) - 1):\n",
    "            layer = GATv2Conv(channel_ls[i], channel_ls[i], heads=2, edge_dim=1)\n",
    "            layers_ls.append(layer)\n",
    "            batch_norm_ls.append(torch_geometric.nn.norm.BatchNorm(self.n_nodes, affine=False))\n",
    "        return nn.ModuleList(layers_ls), nn.ModuleList(batch_norm_ls)\n",
    "\n",
    "    # generate a Batch object for torch geometric\n",
    "    def _create_batch(self, X, A):\n",
    "        data_list = []\n",
    "        for x, adj in zip(X, A):\n",
    "            edge_index = adj.nonzero().t()\n",
    "            edge_weights = adj[edge_index[0], edge_index[1]]\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_weights.view(-1, 1))\n",
    "            data_list.append(data)\n",
    "        return torch_geometric.data.Batch().from_data_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layer that projects adjacency matrix and node embeddings to higher dimensions\n",
    "class AdjacencyStep(nn.Module):\n",
    "    def __init__(self, old_dim, new_dim, channels_ls, dt=1., alpha=0.9, gamma=0.9, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dt = dt\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.gnn = StackedGCN(old_dim, channels_ls, dropout).to(DEVICE)\n",
    "        self.dropout = dropout\n",
    "        self.dim_changer = nn.Parameter(torch.randn((new_dim, old_dim), device=DEVICE))\n",
    "\n",
    "        self.A_dim_changer = nn.Parameter(torch.randn((new_dim, old_dim), device=DEVICE))\n",
    "        self.A_dim_bias = nn.Parameter(torch.randn((new_dim, 1), device=DEVICE))\n",
    "\n",
    "        self.Z_dim_changer = nn.Parameter(torch.randn((channels_ls[-1], new_dim), device=DEVICE))\n",
    "        self.Z_dim_bias = nn.Parameter(torch.randn((new_dim, 1), device=DEVICE))\n",
    "        self.Z_dim_lower = nn.Conv1d(channels_ls[-1], channels_ls[0], kernel_size=1)\n",
    "\n",
    "        self.forget_gate = nn.Parameter(torch.randn(new_dim, device=DEVICE))\n",
    "        self.input_gate = nn.Parameter(torch.randn(new_dim, device=DEVICE))\n",
    "\n",
    "        self.batchnorm_A = torch_geometric.nn.norm.BatchNorm(new_dim)\n",
    "        self.batchnorm_X = torch_geometric.nn.norm.BatchNorm(new_dim)\n",
    "        self.batchnorm_Y = torch_geometric.nn.norm.BatchNorm(new_dim)\n",
    "\n",
    "    def forward(self, X, Y, A):\n",
    "        # update node features with gcn\n",
    "        Z = self.gnn(X, A)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # forget gate from previous adjacency\n",
    "        f = F.sigmoid(self.forget_gate)\n",
    "        forget_A = F.elu(self.A_dim_changer @ A @ self.A_dim_changer.T + self.A_dim_bias)\n",
    "        forget_A = f[:, None] * forget_A\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # input gate from newly learnt node embeddings\n",
    "        i = F.sigmoid(self.input_gate)\n",
    "        input_Z = F.elu(self.dim_changer @ Z @ self.Z_dim_changer + self.Z_dim_bias)\n",
    "        input_Z = i[:, None] * input_Z\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # get new adjacency matrix\n",
    "        new_A = forget_A + input_Z\n",
    "        new_A = self.batchnorm_A(new_A)\n",
    "        new_A = (new_A + torch.transpose(new_A, -1, -2)) / 2\n",
    "        new_A = F.hardtanh(F.hardtanh(new_A, min_val=0), min_val=0)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # update feature embeiddings\n",
    "        Z = torch.transpose(self.Z_dim_lower(torch.transpose(Z, -1, -2)), -1, -2)\n",
    "        Y_temp = Y\n",
    "        Y = self.dim_changer @ (Y + self.dt * (Z - self.alpha * Y - self.gamma * X))\n",
    "        X = self.dim_changer @ (X + self.dt * Y_temp) \n",
    "\n",
    "        # add batch normalisation and dropout at the end\n",
    "        X = self.batchnorm_X(X)\n",
    "        Y = self.batchnorm_Y(Y)   \n",
    "        Y = F.dropout(Y, self.dropout, training=self.training)\n",
    "        X = F.dropout(X, self.dropout, training=self.training)\n",
    "        return X, Y, new_A\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define module combines all the steps together\n",
    "class AdjacencyDimChanger(nn.Module):\n",
    "    def __init__(self, dim_steps, channels_ls):\n",
    "        super().__init__()        \n",
    "        self.layers = nn.ModuleList([AdjacencyStep(dim_steps[i], dim_steps[i+1], channels_ls) for i in range(len(dim_steps)-1)])\n",
    "        \n",
    "    def forward(self, X, Y, A):\n",
    "        adj_ls = [A]\n",
    "        for layer in self.layers:\n",
    "            X, Y, A = layer(X, Y, A)\n",
    "            adj_ls.append(A)\n",
    "        return adj_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions that will be calculated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_loss_fn(up_adj_ls, down_adj_ls,  gamma, epoch, a=0.5, b=1, c=0.2):\n",
    "    total_loss = torch.Tensor([0]).to(DEVICE)\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    # calculate the loss for the upper traingle of the matrix because thats what we care about\n",
    "    final_dim = up_adj_ls[-1].shape[-1]\n",
    "    upper_tri_idx = torch.triu_indices(final_dim, final_dim, offset=1)\n",
    "    final_upper_tri_adj = up_adj_ls[-1][:,upper_tri_idx.unbind()[0], upper_tri_idx.unbind()[1]]\n",
    "    final_upper_tri_other_adj = down_adj_ls[0][:,upper_tri_idx.unbind()[0], upper_tri_idx.unbind()[1]]\n",
    "    final_mse_loss = mse_loss_fn(final_upper_tri_adj, final_upper_tri_other_adj)\n",
    "\n",
    "    # calculate the loss for the remaining intermediate adj. matrices with larger weights on farther steps\n",
    "    n = len(up_adj_ls[:-1])\n",
    "    weights = torch.Tensor([2*(i+1)/(n*(n+1)) for i in range(n)])\n",
    "    intermediate_mse_loss = torch.Tensor([0]).to(DEVICE)\n",
    "    for i, (up_adj, down_adj) in enumerate(zip(up_adj_ls[:-1], down_adj_ls[1:][::-1])):\n",
    "        intermediate_mse_loss = intermediate_mse_loss + weights[i] * mse_loss_fn(up_adj, down_adj)\n",
    "\n",
    "    # sum up the two components of the loss with a weight alpha that decays over epochs\n",
    "    alpha = (1 - np.exp(-c * epoch)) * (a - b) + b\n",
    "    total_loss = total_loss + alpha * final_mse_loss + (1-alpha) * intermediate_mse_loss * (gamma ** 2)\n",
    "    return total_loss\n",
    "\n",
    "def down_loss_fn(down_adj_ls, up_adj_ls):\n",
    "    total_loss = torch.Tensor([0]).to(DEVICE)\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    # calculate the loss for each of the steps with equal weighting\n",
    "    n = len(down_adj_ls[:])\n",
    "    weights = torch.Tensor([1/n for i in range(n)])\n",
    "    for i, (down_adj, up_adj) in enumerate(zip(down_adj_ls[:], up_adj_ls[::-1])):\n",
    "        total_loss = total_loss + weights[i] * mse_loss_fn(down_adj, up_adj)\n",
    "    return total_loss\n",
    "\n",
    "def l1_regularization_loss(models, l1_lambda):\n",
    "    l1_reg_loss = torch.Tensor([0]).to(DEVICE)\n",
    "    for model in models:\n",
    "        all_params = torch.cat([p.view(-1) for p in model.parameters()])\n",
    "        l1_reg_loss = l1_reg_loss + l1_lambda * torch.norm(all_params, 1)\n",
    "    return l1_reg_loss\n",
    "    \n",
    "def reconstruction_loss_fn(gt_adj, pred_adj):\n",
    "    l1_loss_fn = nn.L1Loss()\n",
    "    return l1_loss_fn(gt_adj, pred_adj)\n",
    "\n",
    "def end_adj_loss_calc(adj, opp_adj):\n",
    "    mae_loss_fn = torch.nn.L1Loss()\n",
    "    n = len(adj)\n",
    "\n",
    "    # calculate the upper triangular L1 loss for evaluation only\n",
    "    upper_tri_idx = torch.triu_indices(n, n, offset=1)\n",
    "    upper_tri_adj = adj.detach()[upper_tri_idx.unbind()]\n",
    "    upper_tri_opp_adj = opp_adj.detach()[upper_tri_idx.unbind()]\n",
    "    mae_loss = mae_loss_fn(upper_tri_adj, upper_tri_opp_adj)\n",
    "    return mae_loss.detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, up_changer, down_changer, trainloader, up_optimizer, down_optimizer, reconstruction_optimizer, valloader=None, completed_epochs=0, loss_log=None):\n",
    "\n",
    "    if loss_log is None:\n",
    "        loss_log = {'up': [], 'down': [], 'up_end_mae':[], 'down_end_mae':[], 'val_up_end_mae':[]}\n",
    "\n",
    "    l1_lambda = 0.000001 # for L1 regularization\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # for logging\n",
    "        up_losses = []\n",
    "        up_final_mae_ls = []\n",
    "        down_final_mae_ls = []\n",
    "        down_losses = []\n",
    "        reconstruction_losses = []\n",
    "\n",
    "        # change to training mode\n",
    "        up_changer.train()\n",
    "        down_changer.train()   \n",
    "            \n",
    "        for X_lr, Y_lr, adj_lr, X_hr, Y_hr, adj_hr in tqdm(trainloader, desc=f'Epoch {epoch} Train'):\n",
    "        \n",
    "            # train down changer\n",
    "            freeze_model(up_changer)\n",
    "            unfreeze_model(down_changer)\n",
    "        \n",
    "            down_optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            up_adj_ls = up_changer(X_lr.to(DEVICE), Y_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "            down_adj_ls = down_changer(X_hr.to(DEVICE), Y_hr.to(DEVICE), adj_hr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # calculate total loss from down changer\n",
    "            down_loss = down_loss_fn(down_adj_ls[1:], up_adj_ls[:-1]) + l1_regularization_loss([down_changer], l1_lambda)\n",
    "        \n",
    "            # for printing loss only\n",
    "            down_final_mae_ls.append(end_adj_loss_calc(down_adj_ls[-1].detach(), up_adj_ls[0].detach()))\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # backpropgate results\n",
    "            down_loss.backward()\n",
    "            down_optimizer.step()\n",
    "\n",
    "            # log the results\n",
    "            down_losses.append(down_loss.detach().item())\n",
    "            del down_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # train up changer\n",
    "            unfreeze_model(up_changer)\n",
    "            freeze_model(down_changer)\n",
    "        \n",
    "            up_optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            up_adj_ls = up_changer(X_lr.to(DEVICE), Y_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "            down_adj_ls = down_changer(X_hr.to(DEVICE), Y_hr.to(DEVICE), adj_hr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "            # calculate total loss from up changer\n",
    "            up_loss = up_loss_fn(up_adj_ls[1:], down_adj_ls[:-1], gamma=1, epoch=epoch) + l1_regularization_loss([up_changer], l1_lambda)\n",
    "            \n",
    "            # for printing loss only\n",
    "            up_final_mae_ls.append(end_adj_loss_calc(up_adj_ls[-1].detach(), down_adj_ls[0].detach()))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # backpropagate results\n",
    "            up_loss.backward()\n",
    "            up_optimizer.step()\n",
    "\n",
    "            # log the results\n",
    "            up_losses.append(up_loss.detach().item())\n",
    "            del up_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # train both changer based on down-up reconstruction loss\n",
    "            unfreeze_model(up_changer)\n",
    "            unfreeze_model(down_changer)\n",
    "\n",
    "            reconstruction_optimizer.zero_grad()\n",
    "\n",
    "            # forward pass using the low dimension projection of high resolution adj as input to the up changer\n",
    "            down_adj_end = down_changer(X_hr.to(DEVICE), Y_hr.to(DEVICE), adj_hr.to(DEVICE))[-1]\n",
    "            torch.cuda.empty_cache()\n",
    "            up_adj_end = up_changer(X_lr.to(DEVICE), Y_lr.to(DEVICE), down_adj_end)[-1]\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # calculate the reconstruction loss and backpropagate the results\n",
    "            reconstruction_loss = reconstruction_loss_fn(up_adj_end, adj_hr.to(DEVICE)) + l1_regularization_loss([up_changer, down_changer], l1_lambda) / 2\n",
    "            reconstruction_loss.backward()\n",
    "            reconstruction_optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # log the results\n",
    "            reconstruction_losses.append(reconstruction_loss.detach().item())\n",
    "            del reconstruction_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # do validation if necessary        \n",
    "        if valloader is not None:\n",
    "        \n",
    "            val_up_final_mae_ls = []\n",
    "        \n",
    "            for X_lr, Y_lr, adj_lr, adj_hr in tqdm(valloader, desc=f'Epoch {epoch} Val'):\n",
    "                # change to eval mode\n",
    "                up_changer.eval()\n",
    "                freeze_model(up_changer)\n",
    "\n",
    "\n",
    "                # evaluate the MAE from a single forward pass in up changer on validation data\n",
    "                up_adj_ls = up_changer(X_lr.to(DEVICE), Y_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "                torch.cuda.empty_cache()\n",
    "                val_up_final_mae_ls.append(end_adj_loss_calc(up_adj_ls[-1].detach(), adj_hr.to(DEVICE)))\n",
    "            \n",
    "            epoch_val_up_final_mae = np.mean(val_up_final_mae_ls)\n",
    "            loss_log['val_up_end_mae'].append(epoch_val_up_final_mae)\n",
    "\n",
    "    \n",
    "        # for logging the results    \n",
    "        epoch_up_loss = np.mean(up_losses)\n",
    "        epoch_down_loss = np.mean(down_losses)\n",
    "        epoch_reconstruction_loss = np.mean(reconstruction_losses)\n",
    "        epoch_up_final_mae = np.mean(up_final_mae_ls)\n",
    "        epoch_down_final_mae = np.mean(down_final_mae_ls)\n",
    "        \n",
    "        loss_log['up'].append(epoch_up_loss)\n",
    "        loss_log['down'].append(epoch_down_loss)\n",
    "        loss_log['up_end_mae'].append(epoch_up_final_mae)\n",
    "        loss_log['down_end_mae'].append(epoch_down_final_mae)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'ep{epoch}: DOWN L={epoch_down_loss}, UP L={epoch_up_loss}, RC L={epoch_reconstruction_loss}, DOWN MAE={epoch_down_final_mae}, UP MAE={epoch_up_final_mae}, UP VAL MAE={epoch_val_up_final_mae}')\n",
    "\n",
    "\n",
    "    return up_changer, down_changer, loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "dim_steps = generate_steps(num_steps=8)\n",
    "channels_ls = [32, 64]\n",
    "\n",
    "up_changer = AdjacencyDimChanger(dim_steps, channels_ls).to(DEVICE)\n",
    "down_changer = AdjacencyDimChanger(dim_steps[::-1], channels_ls).to(DEVICE)\n",
    "\n",
    "# define the optimisers\n",
    "up_optimizer = torch.optim.AdamW(up_changer.parameters(), lr=0.002)\n",
    "down_optimizer = torch.optim.AdamW(down_changer.parameters(), lr=0.002)\n",
    "reconstruction_optimizer = torch.optim.AdamW(list(up_changer.parameters()) + list(down_changer.parameters()), lr=0.002)\n",
    "\n",
    "# model size\n",
    "sum(p.numel() for model in [up_changer, down_changer] for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "up_changer, down_changer, loss_log = train(300, up_changer, down_changer, trainloader, up_optimizer, down_optimizer, reconstruction_optimizer, valloader=valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "plt.plot(np.arange(len(loss_log['up_end_mae'])), loss_log['up_end_mae'], label='Training')\n",
    "plt.plot(np.arange(len(loss_log['up_end_mae'])), loss_log['val_up_end_mae'], label='Validation')\n",
    "plt.legend()\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate test predictions\n",
    "testloader = DataLoader(list(zip(lr_X_dim1_test, lr_X_dim3_test, lr_test)), shuffle=False, batch_size=16)\n",
    "\n",
    "up_changer.eval()\n",
    "test_predictions = []\n",
    "for X_lr, Y_lr, adj_lr in tqdm(testloader):\n",
    "    pred = up_changer(X_lr.to(DEVICE), Y_lr.to(DEVICE), adj_lr.to(DEVICE))[-1].detach()\n",
    "    test_predictions.append(pred)\n",
    "test_predictions = torch.cat(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate heat map for predictions\n",
    "generate_heatmap(test_predictions[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare distribution between predictions and training target data\n",
    "generate_histogram(test_predictions.cpu(), hr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(up_changer, testloader, val_adj_hr):\n",
    "    print('begin validation')\n",
    "    up_changer.eval()\n",
    "\n",
    "    # generate predictions for test data\n",
    "    val_predictions = []\n",
    "    for X_lr, Y_lr, adj_lr in tqdm(testloader):\n",
    "        pred = up_changer(X_lr.to(DEVICE), Y_lr.to(DEVICE), adj_lr.to(DEVICE))[-1].detach()\n",
    "        val_predictions.append(pred)\n",
    "    val_predictions = torch.cat(val_predictions)\n",
    "\n",
    "    # evaluate the performance of prediction\n",
    "    return evaluate_predictions(val_predictions, val_adj_hr)\n",
    "\n",
    "def cross_validate(epochs, batch_size, n_fold, X_lr, Y_lr, adj_lr, X_hr, Y_hr, adj_hr):\n",
    "    kf = KFold(n_fold, shuffle=True, random_state=99)\n",
    "    runs_results = []\n",
    "    for train_idx, val_idx in kf.split(X_lr):\n",
    "\n",
    "        # perform upsampling on training data\n",
    "        idxs = upsampling_idx(hr_train[train_idx])\n",
    "        repeats = 4 \n",
    "\n",
    "        train_adj_lr = upsampled_data(adj_lr[train_idx], repeats, idxs)\n",
    "        train_adj_hr = upsampled_data(adj_hr[train_idx], repeats, idxs)\n",
    "        train_X_lr = upsampled_data(X_lr[train_idx], repeats, idxs)\n",
    "        train_X_hr = upsampled_data(X_hr[train_idx], repeats, idxs)\n",
    "        train_Y_lr = upsampled_data(Y_lr[train_idx], repeats, idxs)\n",
    "        train_Y_hr = upsampled_data(Y_hr[train_idx], repeats, idxs)\n",
    "        \n",
    "        # split for validation data\n",
    "        val_X_lr = X_lr[val_idx]\n",
    "        val_Y_lr = Y_lr[val_idx]\n",
    "        val_adj_lr = adj_lr[val_idx]\n",
    "        val_adj_hr = adj_hr[val_idx]\n",
    "\n",
    "        # load into DataLoader\n",
    "        trainloader = DataLoader(list(zip(train_X_lr, train_Y_lr, train_adj_lr, train_X_hr, train_Y_hr, train_adj_hr)), shuffle=True, batch_size=batch_size)\n",
    "        testloader = DataLoader(list(zip(val_X_lr, val_Y_lr, val_adj_lr)), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        # define model\n",
    "        dim_steps = generate_steps(num_steps=8)\n",
    "        channels_ls = [32, 64]\n",
    "        up_changer = AdjacencyDimChanger(dim_steps, channels_ls).to(DEVICE)\n",
    "        down_changer = AdjacencyDimChanger(dim_steps[::-1], channels_ls).to(DEVICE)\n",
    "\n",
    "        # define the optimisers\n",
    "        up_optimizer = torch.optim.AdamW(up_changer.parameters(), lr=0.002)\n",
    "        down_optimizer = torch.optim.AdamW(down_changer.parameters(), lr=0.002)\n",
    "        reconstruction_optimizer = torch.optim.AdamW(list(up_changer.parameters()) + list(down_changer.parameters()), lr=0.002)\n",
    "\n",
    "        # train model\n",
    "        up_changer, down_changer, _ = train(epochs, up_changer, down_changer, trainloader, up_optimizer, down_optimizer, reconstruction_optimizer)\n",
    "\n",
    "        # evaluate model\n",
    "        val_metrics = validation(up_changer, testloader, val_adj_hr)\n",
    "        runs_results.append(val_metrics)\n",
    "\n",
    "    return runs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 3-fold cross validation \n",
    "cv_results = cross_validate(200, 32, 3, lr_X_dim1, lr_X_dim3, lr_train, hr_X_dim1, hr_X_dim3, hr_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
