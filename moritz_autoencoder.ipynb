{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_data_tensor\n",
    "\n",
    "lr_train, lr_test, hr_train = load_data_tensor(\"dgl-icl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randint(0, 3, (2, 2, 2))\n",
    "B = torch.randint(0, 3, (2, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [5, 2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[1] @ B[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [0, 0]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [2, 0]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[2, 4],\n",
       "         [5, 2]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.3388, 0.2025, 0.6895, 0.6105, 0.4464, 0.6495, 0.6894, 0.4389,\n",
      "        0.5773, 0.0000, 0.4907, 0.2030, 0.5973, 0.1080, 0.5209, 0.2436, 0.3475,\n",
      "        0.3142, 0.4680, 0.4328, 0.6836, 0.1751, 0.4156, 0.2886, 0.4879, 0.5107,\n",
      "        0.0409, 0.6092, 0.3186, 0.3773, 0.5973, 0.4236, 0.5157, 0.0000, 0.2851,\n",
      "        0.0880, 0.1500, 0.0387, 0.1536, 0.3912, 0.6067, 0.0019, 0.2677, 0.3644,\n",
      "        0.1659, 0.2804, 0.0000, 0.2246, 0.0000, 0.0168, 0.0154, 0.2328, 0.1669,\n",
      "        0.3741, 0.0000, 0.0000, 0.3887, 0.5526, 0.0068, 0.5261, 0.0912, 0.3366,\n",
      "        0.1353, 0.0618, 0.1170, 0.3364, 0.1876, 0.2191, 0.0557, 0.0009, 0.1510,\n",
      "        0.0694, 0.0450, 0.2801, 0.0509, 0.0000, 0.2265, 0.2669, 0.0807, 0.7052,\n",
      "        0.1387, 0.1906, 0.3906, 0.0000, 0.2254, 0.4377, 0.2353, 0.0193, 0.2244,\n",
      "        0.0000, 0.2057, 0.0000, 0.2625, 0.2219, 0.2037, 0.3883, 0.3881, 0.2607,\n",
      "        0.1453, 0.0633, 0.1122, 0.2182, 0.0832, 0.2598, 0.3783, 0.2654, 0.1461,\n",
      "        0.1611, 0.0000, 0.2819, 0.2389, 0.0932, 0.1854, 0.1902, 0.0600, 0.1816,\n",
      "        0.1807, 0.1965, 0.0000, 0.1960, 0.0478, 0.2142, 0.1958, 0.2131, 0.2212,\n",
      "        0.3995, 0.1081, 0.1138, 0.2157, 0.5276, 0.3078, 0.5082, 0.0000, 0.3310,\n",
      "        0.0000, 0.0000, 0.0381, 0.0000, 0.3947, 0.1104, 0.0050, 0.0026, 0.0000,\n",
      "        0.3583, 0.0076, 0.0837, 0.3221, 0.0000, 0.2426, 0.0574, 0.1624, 0.3104,\n",
      "        0.0935, 0.0737, 0.0134, 0.3363, 0.6702, 0.3994, 0.1218])\n"
     ]
    }
   ],
   "source": [
    "lr_train.shape\n",
    "lr_stack = lr_train.reshape(-1, 160)\n",
    "hr_stack = hr_train.reshape(-1,268)\n",
    "print(lr_stack[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(matrix):\n",
    "    avg = torch.mean(matrix)\n",
    "    std = torch.std(matrix)\n",
    "    return (matrix - avg)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26720, 160])\n"
     ]
    }
   ],
   "source": [
    "lr_train2 = torch.bmm(lr_train, lr_train)\n",
    "lr_stack2 = lr_train2.reshape(-1, 160)\n",
    "hr_train2 = torch.bmm(hr_train, hr_train)\n",
    "hr_stack2 = hr_train2.reshape(-1, 268)\n",
    "print(lr_stack2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26720, 160])\n"
     ]
    }
   ],
   "source": [
    "lr_train2 = torch.bmm(lr_train, lr_train)\n",
    "lr_stack2 = lr_train2.reshape(-1, 160)\n",
    "hr_train2 = torch.bmm(hr_train, hr_train)\n",
    "hr_stack2 = hr_train2.reshape(-1, 268)\n",
    "print(lr_stack2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_hop(adj, hop):\n",
    "    adj_new = adj\n",
    "    for i in range(hop-1):\n",
    "        adj_new = torch.bmm(adj_new, adj)\n",
    "    stack_adj = adj_new.reshape(-1,adj.shape[1])\n",
    "    return stack_adj \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14.3806,  7.2528,  6.9529,  ...,  9.8382,  7.9998,  4.3059],\n",
      "        [ 7.2528,  6.7310,  5.0556,  ...,  6.8077,  4.0328,  4.0265],\n",
      "        [ 6.9529,  5.0556,  8.9668,  ...,  7.0718,  5.5102,  2.7241],\n",
      "        ...,\n",
      "        [ 5.8574,  6.6959,  5.0896,  ...,  6.7481,  2.8670,  5.2785],\n",
      "        [ 1.6789,  1.6876,  2.2906,  ...,  2.8670,  7.0863,  4.0678],\n",
      "        [ 4.7431,  5.3005,  4.3073,  ...,  5.2785,  4.0678,  6.6843]])\n",
      "tensor([[14.3806,  7.2528,  6.9529,  ...,  9.8382,  7.9998,  4.3059],\n",
      "        [ 7.2528,  6.7310,  5.0556,  ...,  6.8077,  4.0328,  4.0265],\n",
      "        [ 6.9529,  5.0556,  8.9668,  ...,  7.0718,  5.5102,  2.7241],\n",
      "        ...,\n",
      "        [ 5.8574,  6.6959,  5.0896,  ...,  6.7481,  2.8670,  5.2785],\n",
      "        [ 1.6789,  1.6876,  2.2906,  ...,  2.8670,  7.0863,  4.0678],\n",
      "        [ 4.7431,  5.3005,  4.3073,  ...,  5.2785,  4.0678,  6.6843]])\n"
     ]
    }
   ],
   "source": [
    "print(lr_stack2)\n",
    "print(adj_hop(lr_train,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dim_1 = 64\n",
    "\n",
    "class AutoencoderLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderLR, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(161, 128),\n",
    "        nn.BatchNorm1d(128),              \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,32),\n",
    "        nn.BatchNorm1d(32),\n",
    "        #nn.ReLU(),\n",
    "        #nn.Linear(32, end_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "        #nn.Linear(end_dim, 32),\n",
    "        #nn.BatchNorm1d(32),\n",
    "        #nn.ReLU(),\n",
    "        nn.Linear(32,64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(64,128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128,160),\n",
    "        )\n",
    "    \n",
    "    def encode(self,adj_row, hops):\n",
    "        x = torch.cat([adj_row, hops], dim = 1)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    def decode(self, latent_input):\n",
    "        return self.decoder(latent_input)\n",
    "\n",
    "    def forward(self, adj_row, hoops):\n",
    "        x = self.encode(adj_row, hoops)\n",
    "        x = self.decode(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dim = 16\n",
    "\n",
    "class AutoencoderHR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoencoderHR, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(269, 128),\n",
    "        nn.BatchNorm1d(128),              \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,32),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, end_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "        nn.Linear(end_dim, 32),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(64,128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(128,268),\n",
    "        )\n",
    "    \n",
    "    def encode(self,adj_row, hops):\n",
    "        x = torch.cat([adj_row, hops], dim = 1)\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    def decode(self, latent_input):\n",
    "        return self.decoder(latent_input)\n",
    "\n",
    "    def forward(self, adj_row, hoops):\n",
    "        x = self.encode(adj_row, hoops)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "in_dim = 160\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(160, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128, 64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        self.fc_m = nn.Linear(32,16)\n",
    "        self.fc_std = nn.Linear(32,16)\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(16,32),\n",
    "            # nn.BatchNorm1d(32),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Linear(32,64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 160)\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc_m(h1), self.fc_std(h1)\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu,logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "in_dim = 268\n",
    "\n",
    "class VAEHR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAEHR, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(268, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(256, 128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128,64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(64,32),\n",
    "            # nn.BatchNorm1d(32),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        self.fc_m = nn.Linear(32,16)\n",
    "        self.fc_std = nn.Linear(32,16)\n",
    "        self.decoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(16,32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # nn.Linear(32,64),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            # nn.Linear(64,128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            # nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 268)\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc_m(h1), self.fc_std(h1)\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu,logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, adj_matrix, num_epoch = 200, lr = 0.001, batch_size = 128):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "    for step in range(num_epoch):\n",
    "        n_completed = 0\n",
    "        while n_completed < len(adj_matrix):\n",
    "            optimizer.zero_grad()\n",
    "            batch = adj_matrix[n_completed: n_completed+batch_size]\n",
    "            e, mu, logvar = model(batch)\n",
    "            loss = loss_function(e,batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_completed += batch_size\n",
    "            print(f'completed: {n_completed} loss: {loss.detach()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2825,  0.0442, -0.0079,  ...,  0.4934,  0.1740, -0.4677],\n",
       "        [ 0.0442, -0.0464, -0.3375,  ..., -0.0331, -0.5152, -0.5163],\n",
       "        [-0.0079, -0.3375,  0.3420,  ...,  0.0128, -0.2585, -0.7425],\n",
       "        ...,\n",
       "        [-0.1982, -0.0525, -0.3316,  ..., -0.0434, -0.7177, -0.2988],\n",
       "        [-0.9241, -0.9226, -0.8178,  ..., -0.7177,  0.0153, -0.5091],\n",
       "        [-0.3918, -0.2949, -0.4675,  ..., -0.2988, -0.5091, -0.0545]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardization(lr_stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14.3806,  7.2528,  6.9529,  ...,  9.8382,  7.9998,  4.3059],\n",
      "        [ 7.2528,  6.7310,  5.0556,  ...,  6.8077,  4.0328,  4.0265],\n",
      "        [ 6.9529,  5.0556,  8.9668,  ...,  7.0718,  5.5102,  2.7241],\n",
      "        ...,\n",
      "        [ 5.8574,  6.6959,  5.0896,  ...,  6.7481,  2.8670,  5.2785],\n",
      "        [ 1.6789,  1.6876,  2.2906,  ...,  2.8670,  7.0863,  4.0678],\n",
      "        [ 4.7431,  5.3005,  4.3073,  ...,  5.2785,  4.0678,  6.6843]])\n",
      "tensor([[ 1.2825,  0.0442, -0.0079,  ...,  0.4934,  0.1740, -0.4677],\n",
      "        [ 0.0442, -0.0464, -0.3375,  ..., -0.0331, -0.5152, -0.5163],\n",
      "        [-0.0079, -0.3375,  0.3420,  ...,  0.0128, -0.2585, -0.7425],\n",
      "        ...,\n",
      "        [-0.1982, -0.0525, -0.3316,  ..., -0.0434, -0.7177, -0.2988],\n",
      "        [-0.9241, -0.9226, -0.8178,  ..., -0.7177,  0.0153, -0.5091],\n",
      "        [-0.3918, -0.2949, -0.4675,  ..., -0.2988, -0.5091, -0.0545]])\n"
     ]
    }
   ],
   "source": [
    "print(lr_stack2)\n",
    "norm = standardization(lr_stack2)\n",
    "print(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "lr_test_stack = lr_test.reshape(-1,160)\n",
    "lr_test_stack2 = adj_hop(lr_test,3)\n",
    "print(lr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51520\n",
      "completed: 128 loss: 8604.6025390625\n",
      "completed: 256 loss: 7045.70703125\n",
      "completed: 384 loss: 5989.79052734375\n",
      "completed: 512 loss: 7615.0439453125\n",
      "completed: 640 loss: 5118.90966796875\n",
      "completed: 768 loss: 6947.1083984375\n",
      "completed: 896 loss: 8142.88330078125\n",
      "completed: 1024 loss: 7702.5146484375\n",
      "completed: 1152 loss: 32128.73828125\n",
      "completed: 1280 loss: 7326.56689453125\n",
      "completed: 1408 loss: 5795.18994140625\n",
      "completed: 1536 loss: 5608.68408203125\n",
      "completed: 1664 loss: 30801.150390625\n",
      "completed: 1792 loss: 61101.9296875\n",
      "completed: 1920 loss: 5775.62548828125\n",
      "completed: 2048 loss: 71539.4140625\n",
      "completed: 2176 loss: 17743.908203125\n",
      "completed: 2304 loss: 4083.2783203125\n",
      "completed: 2432 loss: 3677.1044921875\n",
      "completed: 2560 loss: 5544.966796875\n",
      "completed: 2688 loss: 3745.160400390625\n",
      "completed: 2816 loss: 12075.7724609375\n",
      "completed: 2944 loss: 8532.998046875\n",
      "completed: 3072 loss: 3908.572265625\n",
      "completed: 3200 loss: 3264.594970703125\n",
      "completed: 3328 loss: 6038.6953125\n",
      "completed: 3456 loss: 5198.173828125\n",
      "completed: 3584 loss: 3978.810302734375\n",
      "completed: 3712 loss: 4020.111572265625\n",
      "completed: 3840 loss: 3862.0673828125\n",
      "completed: 3968 loss: 5890.9365234375\n",
      "completed: 4096 loss: 7545.44873046875\n",
      "completed: 4224 loss: 5141.15673828125\n",
      "completed: 4352 loss: 6758.06005859375\n",
      "completed: 4480 loss: 2316.846435546875\n",
      "completed: 4608 loss: 6700.2958984375\n",
      "completed: 4736 loss: 4164.30078125\n",
      "completed: 4864 loss: 4125.916015625\n",
      "completed: 4992 loss: 4730.68115234375\n",
      "completed: 5120 loss: 5787.81494140625\n",
      "completed: 5248 loss: 2637.9599609375\n",
      "completed: 5376 loss: 25763.50390625\n",
      "completed: 5504 loss: 23294.685546875\n",
      "completed: 5632 loss: 4226.04638671875\n",
      "completed: 5760 loss: 23904.771484375\n",
      "completed: 5888 loss: 3471.47265625\n",
      "completed: 6016 loss: 3846.54736328125\n",
      "completed: 6144 loss: 3476.635986328125\n",
      "completed: 6272 loss: 2664.467041015625\n",
      "completed: 6400 loss: 3820.5224609375\n",
      "completed: 6528 loss: 4900.50341796875\n",
      "completed: 6656 loss: 57144.16015625\n",
      "completed: 6784 loss: 89402.484375\n",
      "completed: 6912 loss: 73020.3828125\n",
      "completed: 7040 loss: 42465.3515625\n",
      "completed: 7168 loss: 3082.388671875\n",
      "completed: 7296 loss: 28282.501953125\n",
      "completed: 7424 loss: 25843.236328125\n",
      "completed: 7552 loss: 58648.984375\n",
      "completed: 7680 loss: 5000.62890625\n",
      "completed: 7808 loss: 113516.7109375\n",
      "completed: 7936 loss: 32405.513671875\n",
      "completed: 8064 loss: 7034.4541015625\n",
      "completed: 8192 loss: 7944.9677734375\n",
      "completed: 8320 loss: 9421.541015625\n",
      "completed: 8448 loss: 10640.3759765625\n",
      "completed: 8576 loss: 7691.298828125\n",
      "completed: 8704 loss: 19357.4921875\n",
      "completed: 8832 loss: 29082.568359375\n",
      "completed: 8960 loss: 7699.74365234375\n",
      "completed: 9088 loss: 14772.6337890625\n",
      "completed: 9216 loss: 7817.39501953125\n",
      "completed: 9344 loss: 7203.76806640625\n",
      "completed: 9472 loss: 6451.45166015625\n",
      "completed: 9600 loss: 2488.36767578125\n",
      "completed: 9728 loss: 8327.2958984375\n",
      "completed: 9856 loss: 5384.9365234375\n",
      "completed: 9984 loss: 5899.50732421875\n",
      "completed: 10112 loss: 8389.4326171875\n",
      "completed: 10240 loss: 5804.6298828125\n",
      "completed: 10368 loss: 4396.41259765625\n",
      "completed: 10496 loss: 2430.279296875\n",
      "completed: 10624 loss: 3367.62890625\n",
      "completed: 10752 loss: 6515.04296875\n",
      "completed: 10880 loss: 4432.0205078125\n",
      "completed: 11008 loss: 4490.755859375\n",
      "completed: 11136 loss: 8788.935546875\n",
      "completed: 11264 loss: 9457.9619140625\n",
      "completed: 11392 loss: 4843.6689453125\n",
      "completed: 11520 loss: 23914.779296875\n",
      "completed: 11648 loss: 2651.63525390625\n",
      "completed: 11776 loss: 3128.56787109375\n",
      "completed: 11904 loss: 10739.88671875\n",
      "completed: 12032 loss: 22824.974609375\n",
      "completed: 12160 loss: 5411.37890625\n",
      "completed: 12288 loss: 4468.8359375\n",
      "completed: 12416 loss: 4083.8994140625\n",
      "completed: 12544 loss: 4192.7666015625\n",
      "completed: 12672 loss: 3717.961181640625\n",
      "completed: 12800 loss: 11619.4609375\n",
      "completed: 12928 loss: 19954.099609375\n",
      "completed: 13056 loss: 18345.634765625\n",
      "completed: 13184 loss: 4905.13623046875\n",
      "completed: 13312 loss: 4349.7197265625\n",
      "completed: 13440 loss: 3944.318115234375\n",
      "completed: 13568 loss: 11443.96875\n",
      "completed: 13696 loss: 11223.0302734375\n",
      "completed: 13824 loss: 3618.52490234375\n",
      "completed: 13952 loss: 5991.53173828125\n",
      "completed: 14080 loss: 5505.65185546875\n",
      "completed: 14208 loss: 3246.56494140625\n",
      "completed: 14336 loss: 4064.77978515625\n",
      "completed: 14464 loss: 3066.36376953125\n",
      "completed: 14592 loss: 2784.87939453125\n",
      "completed: 14720 loss: 8801.6025390625\n",
      "completed: 14848 loss: 2376.83349609375\n",
      "completed: 14976 loss: 6011.1982421875\n",
      "completed: 15104 loss: 11770.833984375\n",
      "completed: 15232 loss: 6061.4775390625\n",
      "completed: 15360 loss: 3734.3095703125\n",
      "completed: 15488 loss: 28627.2890625\n",
      "completed: 15616 loss: 6128.177734375\n",
      "completed: 15744 loss: 5828.85400390625\n",
      "completed: 15872 loss: 4722.791015625\n",
      "completed: 16000 loss: 2398.041015625\n",
      "completed: 16128 loss: 8828.291015625\n",
      "completed: 16256 loss: 61474.1484375\n",
      "completed: 16384 loss: 39064.046875\n",
      "completed: 16512 loss: 5886.55810546875\n",
      "completed: 16640 loss: 6466.23974609375\n",
      "completed: 16768 loss: 9388.41015625\n",
      "completed: 16896 loss: 3317.584716796875\n",
      "completed: 17024 loss: 3414.97705078125\n",
      "completed: 17152 loss: 100024.4140625\n",
      "completed: 17280 loss: 978457.5\n",
      "completed: 17408 loss: 45298.23828125\n",
      "completed: 17536 loss: 5807.7841796875\n",
      "completed: 17664 loss: 5438.81591796875\n",
      "completed: 17792 loss: 5241.701171875\n",
      "completed: 17920 loss: 9122.1474609375\n",
      "completed: 128 loss: 8892.037109375\n",
      "completed: 256 loss: 5446.58642578125\n",
      "completed: 384 loss: 5327.04296875\n",
      "completed: 512 loss: 5762.7783203125\n",
      "completed: 640 loss: 5413.77978515625\n",
      "completed: 768 loss: 11730.8125\n",
      "completed: 896 loss: 15475.9765625\n",
      "completed: 1024 loss: 8064.2822265625\n",
      "completed: 1152 loss: 18038.30859375\n",
      "completed: 1280 loss: 4742.978515625\n",
      "completed: 1408 loss: 11875.2138671875\n",
      "completed: 1536 loss: 4872.42431640625\n",
      "completed: 1664 loss: 14480.7080078125\n",
      "completed: 1792 loss: 36310.39453125\n",
      "completed: 1920 loss: 3160.6796875\n",
      "completed: 2048 loss: 46575.94921875\n",
      "completed: 2176 loss: 7462.4462890625\n",
      "completed: 2304 loss: 6278.533203125\n",
      "completed: 2432 loss: 7732.87841796875\n",
      "completed: 2560 loss: 11496.4423828125\n",
      "completed: 2688 loss: 8013.76611328125\n",
      "completed: 2816 loss: 5139.91748046875\n",
      "completed: 2944 loss: 3629.187744140625\n",
      "completed: 3072 loss: 5107.18017578125\n",
      "completed: 3200 loss: 2394.27099609375\n",
      "completed: 3328 loss: 9950.5400390625\n",
      "completed: 3456 loss: 9150.2333984375\n",
      "completed: 3584 loss: 7239.8349609375\n",
      "completed: 3712 loss: 4559.76318359375\n",
      "completed: 3840 loss: 6456.87255859375\n",
      "completed: 3968 loss: 9744.80859375\n",
      "completed: 4096 loss: 11800.5478515625\n",
      "completed: 4224 loss: 5067.91015625\n",
      "completed: 4352 loss: 4325.169921875\n",
      "completed: 4480 loss: 3928.203857421875\n",
      "completed: 4608 loss: 11395.79296875\n",
      "completed: 4736 loss: 3366.53857421875\n",
      "completed: 4864 loss: 4037.394287109375\n",
      "completed: 4992 loss: 7641.990234375\n",
      "completed: 5120 loss: 10276.7099609375\n",
      "completed: 5248 loss: 3222.173583984375\n",
      "completed: 5376 loss: 17774.9609375\n",
      "completed: 5504 loss: 15677.16015625\n",
      "completed: 5632 loss: 4373.0205078125\n",
      "completed: 5760 loss: 17016.484375\n",
      "completed: 5888 loss: 5891.60595703125\n",
      "completed: 6016 loss: 3103.500244140625\n",
      "completed: 6144 loss: 3138.409423828125\n",
      "completed: 6272 loss: 3022.4345703125\n",
      "completed: 6400 loss: 3325.066650390625\n",
      "completed: 6528 loss: 8470.3564453125\n",
      "completed: 6656 loss: 44948.03125\n",
      "completed: 6784 loss: 77407.5390625\n",
      "completed: 6912 loss: 61705.5625\n",
      "completed: 7040 loss: 35091.7578125\n",
      "completed: 7168 loss: 2993.365478515625\n",
      "completed: 7296 loss: 22238.7421875\n",
      "completed: 7424 loss: 20518.041015625\n",
      "completed: 7552 loss: 50375.4140625\n",
      "completed: 7680 loss: 4874.37158203125\n",
      "completed: 7808 loss: 103618.8359375\n",
      "completed: 7936 loss: 26151.486328125\n",
      "completed: 8064 loss: 4463.79638671875\n",
      "completed: 8192 loss: 8452.603515625\n",
      "completed: 8320 loss: 9816.0400390625\n",
      "completed: 8448 loss: 11053.8447265625\n",
      "completed: 8576 loss: 7034.1513671875\n",
      "completed: 8704 loss: 14435.0419921875\n",
      "completed: 8832 loss: 23579.517578125\n",
      "completed: 8960 loss: 7603.83935546875\n",
      "completed: 9088 loss: 13947.53125\n",
      "completed: 9216 loss: 4001.28759765625\n",
      "completed: 9344 loss: 7002.4306640625\n",
      "completed: 9472 loss: 5869.47607421875\n",
      "completed: 9600 loss: 2790.716796875\n",
      "completed: 9728 loss: 7889.5380859375\n",
      "completed: 9856 loss: 5005.07080078125\n",
      "completed: 9984 loss: 5454.4560546875\n",
      "completed: 10112 loss: 9068.9228515625\n",
      "completed: 10240 loss: 6680.78271484375\n",
      "completed: 10368 loss: 4718.61328125\n",
      "completed: 10496 loss: 2834.825439453125\n",
      "completed: 10624 loss: 3165.836181640625\n",
      "completed: 10752 loss: 5460.7548828125\n",
      "completed: 10880 loss: 5416.8984375\n",
      "completed: 11008 loss: 5495.552734375\n",
      "completed: 11136 loss: 6769.8896484375\n",
      "completed: 11264 loss: 7132.53955078125\n",
      "completed: 11392 loss: 4075.22265625\n",
      "completed: 11520 loss: 20420.369140625\n",
      "completed: 11648 loss: 3034.231689453125\n",
      "completed: 11776 loss: 3637.071533203125\n",
      "completed: 11904 loss: 8654.611328125\n",
      "completed: 12032 loss: 19817.333984375\n",
      "completed: 12160 loss: 6656.396484375\n",
      "completed: 12288 loss: 5584.484375\n",
      "completed: 12416 loss: 4856.00927734375\n",
      "completed: 12544 loss: 4812.21728515625\n",
      "completed: 12672 loss: 3776.62646484375\n",
      "completed: 12800 loss: 10146.6806640625\n",
      "completed: 12928 loss: 17727.998046875\n",
      "completed: 13056 loss: 16383.580078125\n",
      "completed: 13184 loss: 4301.76025390625\n",
      "completed: 13312 loss: 4711.7744140625\n",
      "completed: 13440 loss: 3234.2236328125\n",
      "completed: 13568 loss: 11026.716796875\n",
      "completed: 13696 loss: 10098.2705078125\n",
      "completed: 13824 loss: 3487.22802734375\n",
      "completed: 13952 loss: 6676.30419921875\n",
      "completed: 14080 loss: 6094.9970703125\n",
      "completed: 14208 loss: 3416.6708984375\n",
      "completed: 14336 loss: 4470.8564453125\n",
      "completed: 14464 loss: 3337.0390625\n",
      "completed: 14592 loss: 2775.708251953125\n",
      "completed: 14720 loss: 9342.478515625\n",
      "completed: 14848 loss: 2694.194580078125\n",
      "completed: 14976 loss: 5170.294921875\n",
      "completed: 15104 loss: 10494.4609375\n",
      "completed: 15232 loss: 6731.0087890625\n",
      "completed: 15360 loss: 3603.052001953125\n",
      "completed: 15488 loss: 27191.7578125\n",
      "completed: 15616 loss: 5804.4482421875\n",
      "completed: 15744 loss: 6400.916015625\n",
      "completed: 15872 loss: 5111.88916015625\n",
      "completed: 16000 loss: 2466.679931640625\n",
      "completed: 16128 loss: 9602.15625\n",
      "completed: 16256 loss: 58070.87890625\n",
      "completed: 16384 loss: 36996.55859375\n",
      "completed: 16512 loss: 6563.5498046875\n",
      "completed: 16640 loss: 7114.662109375\n",
      "completed: 16768 loss: 8829.9658203125\n",
      "completed: 16896 loss: 3300.79248046875\n",
      "completed: 17024 loss: 3471.498046875\n",
      "completed: 17152 loss: 97645.8671875\n",
      "completed: 17280 loss: 972474.5625\n",
      "completed: 17408 loss: 44854.42578125\n",
      "completed: 17536 loss: 5667.43701171875\n",
      "completed: 17664 loss: 5536.80029296875\n",
      "completed: 17792 loss: 5294.318359375\n",
      "completed: 17920 loss: 8402.896484375\n",
      "completed: 128 loss: 7921.2255859375\n",
      "completed: 256 loss: 4868.845703125\n",
      "completed: 384 loss: 4679.1787109375\n",
      "completed: 512 loss: 5215.99560546875\n",
      "completed: 640 loss: 4789.3271484375\n",
      "completed: 768 loss: 10689.3125\n",
      "completed: 896 loss: 14396.16796875\n",
      "completed: 1024 loss: 7303.30078125\n",
      "completed: 1152 loss: 18137.13671875\n",
      "completed: 1280 loss: 4392.14013671875\n",
      "completed: 1408 loss: 11092.380859375\n",
      "completed: 1536 loss: 4387.85107421875\n",
      "completed: 1664 loss: 14325.869140625\n",
      "completed: 1792 loss: 36834.1484375\n",
      "completed: 1920 loss: 3051.9873046875\n",
      "completed: 2048 loss: 47558.80078125\n",
      "completed: 2176 loss: 7360.68994140625\n",
      "completed: 2304 loss: 5774.71875\n",
      "completed: 2432 loss: 7082.73876953125\n",
      "completed: 2560 loss: 10733.3173828125\n",
      "completed: 2688 loss: 7587.62744140625\n",
      "completed: 2816 loss: 5095.33154296875\n",
      "completed: 2944 loss: 3605.736572265625\n",
      "completed: 3072 loss: 4752.7412109375\n",
      "completed: 3200 loss: 2243.229736328125\n",
      "completed: 3328 loss: 9553.2919921875\n",
      "completed: 3456 loss: 8796.9453125\n",
      "completed: 3584 loss: 6919.64794921875\n",
      "completed: 3712 loss: 4363.36376953125\n",
      "completed: 3840 loss: 6323.72998046875\n",
      "completed: 3968 loss: 9578.64453125\n",
      "completed: 4096 loss: 11687.779296875\n",
      "completed: 4224 loss: 4800.5810546875\n",
      "completed: 4352 loss: 4124.34619140625\n",
      "completed: 4480 loss: 4096.171875\n",
      "completed: 4608 loss: 11697.1533203125\n",
      "completed: 4736 loss: 3217.3330078125\n",
      "completed: 4864 loss: 3776.7666015625\n",
      "completed: 4992 loss: 7867.58642578125\n",
      "completed: 5120 loss: 10712.03515625\n",
      "completed: 5248 loss: 3322.9921875\n",
      "completed: 5376 loss: 15847.912109375\n",
      "completed: 5504 loss: 14012.2490234375\n",
      "completed: 5632 loss: 4262.15478515625\n",
      "completed: 5760 loss: 16187.7666015625\n",
      "completed: 5888 loss: 6096.48828125\n",
      "completed: 6016 loss: 3035.638671875\n",
      "completed: 6144 loss: 3178.73583984375\n",
      "completed: 6272 loss: 3049.952392578125\n",
      "completed: 6400 loss: 3349.306396484375\n",
      "completed: 6528 loss: 8911.5078125\n",
      "completed: 6656 loss: 43142.96484375\n",
      "completed: 6784 loss: 74803.3515625\n",
      "completed: 6912 loss: 59625.6796875\n",
      "completed: 7040 loss: 33848.3828125\n",
      "completed: 7168 loss: 3093.697265625\n",
      "completed: 7296 loss: 21134.962890625\n",
      "completed: 7424 loss: 20012.7109375\n",
      "completed: 7552 loss: 49815.23828125\n",
      "completed: 7680 loss: 5013.92333984375\n",
      "completed: 7808 loss: 102705.1953125\n",
      "completed: 7936 loss: 25724.548828125\n",
      "completed: 8064 loss: 4392.05712890625\n",
      "completed: 8192 loss: 8476.4951171875\n",
      "completed: 8320 loss: 9838.0947265625\n",
      "completed: 8448 loss: 10924.212890625\n",
      "completed: 8576 loss: 6966.66162109375\n",
      "completed: 8704 loss: 14619.154296875\n",
      "completed: 8832 loss: 23463.3046875\n",
      "completed: 8960 loss: 7416.65283203125\n",
      "completed: 9088 loss: 13999.138671875\n",
      "completed: 9216 loss: 3687.6064453125\n",
      "completed: 9344 loss: 6752.42822265625\n",
      "completed: 9472 loss: 5670.62451171875\n",
      "completed: 9600 loss: 2632.095947265625\n",
      "completed: 9728 loss: 7696.36083984375\n",
      "completed: 9856 loss: 4765.0888671875\n",
      "completed: 9984 loss: 5240.62548828125\n",
      "completed: 10112 loss: 8810.5498046875\n",
      "completed: 10240 loss: 6528.69384765625\n",
      "completed: 10368 loss: 4709.40087890625\n",
      "completed: 10496 loss: 2775.61767578125\n",
      "completed: 10624 loss: 3135.313720703125\n",
      "completed: 10752 loss: 5546.03759765625\n",
      "completed: 10880 loss: 5322.10986328125\n",
      "completed: 11008 loss: 5478.6572265625\n",
      "completed: 11136 loss: 6568.171875\n",
      "completed: 11264 loss: 6887.119140625\n",
      "completed: 11392 loss: 3912.0185546875\n",
      "completed: 11520 loss: 20382.37890625\n",
      "completed: 11648 loss: 3126.178466796875\n",
      "completed: 11776 loss: 3657.100341796875\n",
      "completed: 11904 loss: 8117.68505859375\n",
      "completed: 12032 loss: 19499.814453125\n",
      "completed: 12160 loss: 6765.19677734375\n",
      "completed: 12288 loss: 5501.9619140625\n",
      "completed: 12416 loss: 4761.23193359375\n",
      "completed: 12544 loss: 4845.58740234375\n",
      "completed: 12672 loss: 3786.576171875\n",
      "completed: 12800 loss: 9825.966796875\n",
      "completed: 12928 loss: 17507.541015625\n",
      "completed: 13056 loss: 16071.5400390625\n",
      "completed: 13184 loss: 4291.865234375\n",
      "completed: 13312 loss: 4672.49658203125\n",
      "completed: 13440 loss: 3058.96337890625\n",
      "completed: 13568 loss: 10798.2421875\n",
      "completed: 13696 loss: 10023.5947265625\n",
      "completed: 13824 loss: 3421.444091796875\n",
      "completed: 13952 loss: 6744.970703125\n",
      "completed: 14080 loss: 6093.0791015625\n",
      "completed: 14208 loss: 3421.609130859375\n",
      "completed: 14336 loss: 4369.37646484375\n",
      "completed: 14464 loss: 3259.00537109375\n",
      "completed: 14592 loss: 2770.355224609375\n",
      "completed: 14720 loss: 9286.603515625\n",
      "completed: 14848 loss: 2736.23583984375\n",
      "completed: 14976 loss: 4826.01953125\n",
      "completed: 15104 loss: 10292.046875\n",
      "completed: 15232 loss: 6824.91650390625\n",
      "completed: 15360 loss: 3505.510986328125\n",
      "completed: 15488 loss: 27118.705078125\n",
      "completed: 15616 loss: 5529.46875\n",
      "completed: 15744 loss: 6539.666015625\n",
      "completed: 15872 loss: 5028.84716796875\n",
      "completed: 16000 loss: 2382.707275390625\n",
      "completed: 16128 loss: 9547.283203125\n",
      "completed: 16256 loss: 57590.328125\n",
      "completed: 16384 loss: 36189.52734375\n",
      "completed: 16512 loss: 6699.68798828125\n",
      "completed: 16640 loss: 7234.58837890625\n",
      "completed: 16768 loss: 8865.39453125\n",
      "completed: 16896 loss: 3051.6474609375\n",
      "completed: 17024 loss: 3327.12939453125\n",
      "completed: 17152 loss: 95962.6640625\n",
      "completed: 17280 loss: 971325.5\n",
      "completed: 17408 loss: 44806.203125\n",
      "completed: 17536 loss: 5307.6943359375\n",
      "completed: 17664 loss: 5448.6728515625\n",
      "completed: 17792 loss: 5196.19140625\n",
      "completed: 17920 loss: 8341.056640625\n",
      "completed: 128 loss: 7662.4736328125\n",
      "completed: 256 loss: 4664.03369140625\n",
      "completed: 384 loss: 4442.876953125\n",
      "completed: 512 loss: 4899.7841796875\n",
      "completed: 640 loss: 4633.12158203125\n",
      "completed: 768 loss: 10497.0546875\n",
      "completed: 896 loss: 14153.3984375\n",
      "completed: 1024 loss: 7001.853515625\n",
      "completed: 1152 loss: 18061.326171875\n",
      "completed: 1280 loss: 4213.74560546875\n",
      "completed: 1408 loss: 10775.8994140625\n",
      "completed: 1536 loss: 4250.94140625\n",
      "completed: 1664 loss: 14196.62109375\n",
      "completed: 1792 loss: 36812.8046875\n",
      "completed: 1920 loss: 2972.626953125\n",
      "completed: 2048 loss: 47819.3046875\n",
      "completed: 2176 loss: 7374.3310546875\n",
      "completed: 2304 loss: 5568.85546875\n",
      "completed: 2432 loss: 6926.751953125\n",
      "completed: 2560 loss: 10398.7099609375\n",
      "completed: 2688 loss: 7360.17138671875\n",
      "completed: 2816 loss: 5023.04638671875\n",
      "completed: 2944 loss: 3429.7373046875\n",
      "completed: 3072 loss: 4619.8466796875\n",
      "completed: 3200 loss: 2186.95703125\n",
      "completed: 3328 loss: 9369.41796875\n",
      "completed: 3456 loss: 8720.77734375\n",
      "completed: 3584 loss: 6821.953125\n",
      "completed: 3712 loss: 4338.08056640625\n",
      "completed: 3840 loss: 6266.8017578125\n",
      "completed: 3968 loss: 9544.0908203125\n",
      "completed: 4096 loss: 11762.0087890625\n",
      "completed: 4224 loss: 4812.599609375\n",
      "completed: 4352 loss: 3969.2783203125\n",
      "completed: 4480 loss: 4069.741455078125\n",
      "completed: 4608 loss: 11753.5224609375\n",
      "completed: 4736 loss: 3215.361572265625\n",
      "completed: 4864 loss: 3864.70751953125\n",
      "completed: 4992 loss: 8042.85498046875\n",
      "completed: 5120 loss: 11064.673828125\n",
      "completed: 5248 loss: 3303.6142578125\n",
      "completed: 5376 loss: 14990.11328125\n",
      "completed: 5504 loss: 13154.1171875\n",
      "completed: 5632 loss: 4393.2763671875\n",
      "completed: 5760 loss: 15737.6923828125\n",
      "completed: 5888 loss: 6315.2001953125\n",
      "completed: 6016 loss: 3041.509521484375\n",
      "completed: 6144 loss: 3225.0322265625\n",
      "completed: 6272 loss: 3096.74267578125\n",
      "completed: 6400 loss: 3273.01318359375\n",
      "completed: 6528 loss: 8838.037109375\n",
      "completed: 6656 loss: 41401.8828125\n",
      "completed: 6784 loss: 73811.984375\n",
      "completed: 6912 loss: 58384.0234375\n",
      "completed: 7040 loss: 33369.03515625\n",
      "completed: 7168 loss: 3124.538818359375\n",
      "completed: 7296 loss: 20857.4375\n",
      "completed: 7424 loss: 19286.771484375\n",
      "completed: 7552 loss: 48503.578125\n",
      "completed: 7680 loss: 5064.6787109375\n",
      "completed: 7808 loss: 101325.15625\n",
      "completed: 7936 loss: 24726.25390625\n",
      "completed: 8064 loss: 4263.45263671875\n",
      "completed: 8192 loss: 8450.6513671875\n",
      "completed: 8320 loss: 9840.5517578125\n",
      "completed: 8448 loss: 10911.9248046875\n",
      "completed: 8576 loss: 7003.14453125\n",
      "completed: 8704 loss: 13910.138671875\n",
      "completed: 8832 loss: 22927.330078125\n",
      "completed: 8960 loss: 7416.1240234375\n",
      "completed: 9088 loss: 13976.3251953125\n",
      "completed: 9216 loss: 3335.25439453125\n",
      "completed: 9344 loss: 6799.5419921875\n",
      "completed: 9472 loss: 5757.75634765625\n",
      "completed: 9600 loss: 2703.83251953125\n",
      "completed: 9728 loss: 7658.13916015625\n",
      "completed: 9856 loss: 4770.365234375\n",
      "completed: 9984 loss: 5307.845703125\n",
      "completed: 10112 loss: 8832.1318359375\n",
      "completed: 10240 loss: 6483.017578125\n",
      "completed: 10368 loss: 4613.47607421875\n",
      "completed: 10496 loss: 2780.9443359375\n",
      "completed: 10624 loss: 3026.7255859375\n",
      "completed: 10752 loss: 5252.55224609375\n",
      "completed: 10880 loss: 5410.607421875\n",
      "completed: 11008 loss: 5512.79248046875\n",
      "completed: 11136 loss: 6198.92529296875\n",
      "completed: 11264 loss: 6660.060546875\n",
      "completed: 11392 loss: 3832.18212890625\n",
      "completed: 11520 loss: 19947.947265625\n",
      "completed: 11648 loss: 3026.362548828125\n",
      "completed: 11776 loss: 3666.01123046875\n",
      "completed: 11904 loss: 7824.1064453125\n",
      "completed: 12032 loss: 19055.82421875\n",
      "completed: 12160 loss: 6950.841796875\n",
      "completed: 12288 loss: 5612.5400390625\n",
      "completed: 12416 loss: 4815.24267578125\n",
      "completed: 12544 loss: 4819.58251953125\n",
      "completed: 12672 loss: 3645.82666015625\n",
      "completed: 12800 loss: 9542.8896484375\n",
      "completed: 12928 loss: 17270.9140625\n",
      "completed: 13056 loss: 15797.7109375\n",
      "completed: 13184 loss: 4113.7763671875\n",
      "completed: 13312 loss: 4626.7294921875\n",
      "completed: 13440 loss: 2962.960205078125\n",
      "completed: 13568 loss: 10810.5478515625\n",
      "completed: 13696 loss: 9875.3740234375\n",
      "completed: 13824 loss: 3257.357666015625\n",
      "completed: 13952 loss: 6828.927734375\n",
      "completed: 14080 loss: 6163.9658203125\n",
      "completed: 14208 loss: 3480.85546875\n",
      "completed: 14336 loss: 4427.12109375\n",
      "completed: 14464 loss: 3324.25634765625\n",
      "completed: 14592 loss: 2607.1298828125\n",
      "completed: 14720 loss: 9220.931640625\n",
      "completed: 14848 loss: 2802.802490234375\n",
      "completed: 14976 loss: 4763.35009765625\n",
      "completed: 15104 loss: 9899.0986328125\n",
      "completed: 15232 loss: 6917.2421875\n",
      "completed: 15360 loss: 3443.071533203125\n",
      "completed: 15488 loss: 26870.04296875\n",
      "completed: 15616 loss: 5053.47998046875\n",
      "completed: 15744 loss: 6612.72265625\n",
      "completed: 15872 loss: 5169.18603515625\n",
      "completed: 16000 loss: 2389.3134765625\n",
      "completed: 16128 loss: 9664.06640625\n",
      "completed: 16256 loss: 57107.984375\n",
      "completed: 16384 loss: 35354.140625\n",
      "completed: 16512 loss: 6763.97265625\n",
      "completed: 16640 loss: 7414.01416015625\n",
      "completed: 16768 loss: 8655.5361328125\n",
      "completed: 16896 loss: 2999.960693359375\n",
      "completed: 17024 loss: 3438.010498046875\n",
      "completed: 17152 loss: 94275.671875\n",
      "completed: 17280 loss: 969169.5\n",
      "completed: 17408 loss: 44417.05078125\n",
      "completed: 17536 loss: 5249.373046875\n",
      "completed: 17664 loss: 5223.08349609375\n",
      "completed: 17792 loss: 5156.7587890625\n",
      "completed: 17920 loss: 8396.8984375\n",
      "completed: 128 loss: 7687.6796875\n",
      "completed: 256 loss: 4623.015625\n",
      "completed: 384 loss: 4537.86083984375\n",
      "completed: 512 loss: 4823.640625\n",
      "completed: 640 loss: 4645.771484375\n",
      "completed: 768 loss: 10570.53125\n",
      "completed: 896 loss: 14057.7314453125\n",
      "completed: 1024 loss: 6937.197265625\n",
      "completed: 1152 loss: 17789.908203125\n",
      "completed: 1280 loss: 4152.91650390625\n",
      "completed: 1408 loss: 10890.18359375\n",
      "completed: 1536 loss: 4159.52001953125\n",
      "completed: 1664 loss: 13796.234375\n",
      "completed: 1792 loss: 36151.38671875\n",
      "completed: 1920 loss: 2909.626953125\n",
      "completed: 2048 loss: 47556.01171875\n",
      "completed: 2176 loss: 7289.53759765625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(params)\n\u001b[1;32m      4\u001b[0m norm \u001b[38;5;241m=\u001b[39m standardization(lr_test_stack2)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, adj_matrix, num_epoch, lr, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m adj_matrix[n_completed: n_completed\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m----> 8\u001b[0m e, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(e,batch, mu, logvar)\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 50\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparametrize(mu,logvar)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z), mu, logvar\n",
      "Cell \u001b[0;32mIn[58], line 39\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     38\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_m(h1), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad_)\n",
    "print(params)\n",
    "norm = standardization(lr_test_stack2)\n",
    "\n",
    "train(model, norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92268\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x160 and 268x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m hr_stack2 \u001b[38;5;241m=\u001b[39m adj_hop(hr_train,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m norm \u001b[38;5;241m=\u001b[39m standardization(lr_test_stack2)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, adj_matrix, num_epoch, lr, batch_size)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m batch \u001b[38;5;241m=\u001b[39m adj_matrix[n_completed: n_completed\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m----> 8\u001b[0m e, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(e,batch, mu, logvar)\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 50\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparametrize(mu,logvar)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z), mu, logvar\n",
      "Cell \u001b[0;32mIn[32], line 38\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_m(h1), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_std(h1)\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/meh23/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x160 and 268x128)"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad_)\n",
    "print(params)\n",
    "hr_stack2 = adj_hop(hr_train,3)\n",
    "norm = standardization(lr_test_stack2)\n",
    "\n",
    "train(model, norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mu, logvar = model.encode(norm)\n",
    "res_lr_1 = model.reparametrize(mu,logvar).reshape(112, 160, 32)\n",
    "torch.save(res_lr_1.detach(), \"model_autoencoder/final_embeddings/encode_lr_test_3.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_autoencoder/autoencoder.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
