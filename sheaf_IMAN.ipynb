{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh, Dropout, Upsample\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.distributions import normal, kl\n",
    "\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE, InnerProductDecoder, ARGVA\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from MatrixVectorizer import MatrixVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "N_SUBJECTS = 167\n",
    "\n",
    "N_LR_NODES = 160\n",
    "\n",
    "N_HR_NODES = 268\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "N_LR_NODES_F = int(N_LR_NODES * (N_LR_NODES-1) / 2)\n",
    "N_HR_NODES_F = int(N_HR_NODES * (N_HR_NODES-1) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafConvLayer(nn.Module):\n",
    "    def __init__(self, n_nodes, d, f_in, f_out=None):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.n_nodes = n_nodes\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        # random init weight matrices\n",
    "        if f_out is None:\n",
    "            f_out = f_in \n",
    "        self.weight1 = nn.Parameter(torch.randn((d, d), device=DEVICE))\n",
    "        self.weight2 = nn.Parameter(torch.randn((f_in, f_out), device=DEVICE))\n",
    "        self.edge_weights = nn.Parameter(torch.randn((n_nodes, n_nodes, d, 2*d), device=DEVICE))\n",
    "\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        kron_prod = torch.kron(torch.eye(self.n_nodes).to(DEVICE), self.weight1)\n",
    "        L = self.sheaf_laplacian(X, adj)\n",
    "        if self.f_out is None:\n",
    "            return X - F.elu(L @ kron_prod @ X @ self.weight2)\n",
    "        else:\n",
    "            return F.elu(L @ kron_prod @ X @ self.weight2)\n",
    "\n",
    "\n",
    "    def sheaf_laplacian(self, X, adj, epsilon=1e-6):\n",
    "        X_reshaped = X.reshape(self.n_nodes, self.d, -1)\n",
    "        idx_pairs = torch.cartesian_prod(torch.arange(self.n_nodes), torch.arange(self.n_nodes))\n",
    "        all_stacked_features = X_reshaped[idx_pairs].reshape(self.n_nodes, self.n_nodes, 2*self.d, -1).to(DEVICE)\n",
    "        lin_trans = F.elu(torch.matmul(self.edge_weights, all_stacked_features))\n",
    "        inner_transpose = torch.transpose(lin_trans, -1, -2)\n",
    "        L_v = -1 * torch.matmul(lin_trans, torch.transpose(inner_transpose, 0, 1))\n",
    "        row_cond = torch.isclose(torch.sum(adj, dim=1), torch.zeros_like(torch.sum(adj, dim=1)))\n",
    "        col_cond = torch.isclose(torch.sum(adj, dim=0), torch.zeros_like(torch.sum(adj, dim=0)))\n",
    "        adj_row_weights = adj / (torch.sum(adj, dim=1)[:, None] + epsilon)\n",
    "        adj_col_weights = adj / (torch.sum(adj, dim=0)[:, None] + epsilon)\n",
    "        # adj_col_weights = torch.where(col_cond[None, :], 0., adj / torch.sum(adj, dim=0)[None, :])\n",
    "        adj_weights = torch.maximum(adj_row_weights * adj_col_weights, torch.zeros_like(adj_row_weights))\n",
    "\n",
    "        adj_diag_weights = adj_row_weights ** 2\n",
    "        diag_blocks = torch.sum(adj_diag_weights[:, :, None, None] * torch.matmul(lin_trans, inner_transpose), dim=1)\n",
    "        L_v[range(self.n_nodes), range(self.n_nodes)] = diag_blocks\n",
    "        return L_v.view(-1, self.n_nodes * self.d)\n",
    "        ### NOTE IGNORE MATRIX NORMALISATION FOR NOW #####\n",
    "        # inv_root_diag_blocks = torch.pow(diag_blocks+epsilon, -1/2)\n",
    "        # normalise_mat = torch.block_diag(*inv_root_diag_blocks)\n",
    "\n",
    "        # return normalise_mat @ L_v.view(-1, self.n_nodes * self.d) @ normalise_mat\n",
    "        ################################################\n",
    "\n",
    "        # laplacian_ls = []\n",
    "        # for v in range(self.n_nodes):\n",
    "        #     L_v = torch.zeros((self.d, self.d)).to(DEVICE)\n",
    "        #     if torch.sum(adj[v]) > 0:\n",
    "        #         for u in range(self.n_nodes):\n",
    "        #             edge_weight = self.edge_weights[v, u]\n",
    "        #             stacked_features = torch.concat((X[v*self.d:(v+1)*self.d], X[u*self.d:(u+1)*self.d]))\n",
    "        #             lin_trans = F.elu(edge_weight @ stacked_features).to(DEVICE)\n",
    "        #             L_v += adj[v, u] * lin_trans @ lin_trans.T\n",
    "        #         laplacian_ls.append(L_v / torch.sum(adj[v]))\n",
    "        #     else:\n",
    "        #         laplacian_ls.append(L_v)\n",
    "\n",
    "        # return torch.block_diag(*laplacian_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafAligner(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, f):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.f = f\n",
    "\n",
    "        self.sheafconv1 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm1 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv2 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm2 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv3 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm3 = BatchNorm(f)\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "\n",
    "        x1 = self.sheafconv1(X, adj)\n",
    "        x1 = F.sigmoid(self.batchnorm1(x1))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "\n",
    "        mean_x1 = x1.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj1 = F.sigmoid(mean_x1 @ mean_x1.T)\n",
    "\n",
    "        x2 = self.sheafconv2(x1, adj1)\n",
    "        x2 = F.sigmoid(self.batchnorm2(x2))\n",
    "        x2 = F.dropout(x2, training=self.training)\n",
    "        \n",
    "        mean_x2 = x2.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj2 = F.sigmoid(mean_x2 @ mean_x2.T)\n",
    "\n",
    "        x3 = self.sheafconv3(x2, adj2)\n",
    "        x3 = F.sigmoid(self.batchnorm3(x3))\n",
    "\n",
    "        mean_x3 = x3.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj3 = F.sigmoid(mean_x3 @ mean_x3.T)\n",
    "\n",
    "        # x3 = torch.cat([x3, x1], dim=1)\n",
    "\n",
    "        # x4 = x3[:, 0:16]\n",
    "        # x5 = x3[:, 16:2*16]\n",
    "\n",
    "\n",
    "        return x3, adj3\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafGenerator(nn.Module):\n",
    "    def __init__(self, d, f):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.f = f\n",
    "        \n",
    "        self.sheafconv1 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm1 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv2 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm2 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv3 = SheafConvLayer(N_LR_NODES, d, f, N_HR_NODES)\n",
    "        self.batchnorm3 = BatchNorm(N_HR_NODES)\n",
    "\n",
    "        self.out_mat = nn.Parameter(torch.randn((N_LR_NODES, 2*N_LR_NODES), device=DEVICE))\n",
    "\n",
    "        self.out_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        x1 = self.sheafconv1(X, adj) # returns (d*lr_n) * f\n",
    "        x1 = F.sigmoid(self.batchnorm1(x1))\n",
    "        x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "\n",
    "        mean_x1 = x1.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj1 = F.sigmoid(mean_x1 @ mean_x1.T)\n",
    "\n",
    "        x2 = self.sheafconv2(x1, adj1) # returns (d*lr_n) * f\n",
    "        x2 = F.sigmoid(self.batchnorm2(x2))\n",
    "        x2 = F.dropout(x2, p=0.1, training=self.training)\n",
    "\n",
    "        mean_x2 = x2.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj2 = F.sigmoid(mean_x2 @ mean_x2.T)\n",
    "\n",
    "        x3 = self.sheafconv3(x2, adj2) # returns (d*lr_n) * hr_n\n",
    "        x3 = F.sigmoid(self.batchnorm3(x3))\n",
    "        x3 = torch.matmul(self.out_mat, x3.clone().detach())\n",
    "        x3 = torch.sigmoid(torch.t(x3) @ adj @ x3)\n",
    "\n",
    "        return (x3 + torch.t(x3)) / 2 # to ensure the matrix is symmetric\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafDiscriminator(nn.Module):\n",
    "    def __init__(self, d, f):\n",
    "        super().__init__()\n",
    "        self.sheafconv1 = SheafConvLayer(N_HR_NODES, d, f)\n",
    "\n",
    "        self.sheafconv2 = SheafConvLayer(N_HR_NODES, d, f, 1)\n",
    "        self.out = torch.nn.Linear(2*N_HR_NODES, 1)\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        x1 = F.sigmoid(self.sheafconv1(X, adj))\n",
    "        x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "        x2 = F.sigmoid(self.sheafconv2(x1, adj))\n",
    "        x3 = F.sigmoid(self.out(x2.flatten()))\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_coor(input, target, epsilon=1e-7):\n",
    "    vx = input - torch.mean(input, dim=(1, 2))[:, None, None]\n",
    "    vy = target - torch.mean(target, dim=(1, 2))[:, None, None]\n",
    "    cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)+epsilon) * torch.sqrt(torch.sum(vy ** 2)+epsilon)+epsilon)\n",
    "    return cost\n",
    "\n",
    "def GT_loss(target, predicted):\n",
    "\n",
    "    # l1_loss\n",
    "    l1_loss = torch.nn.L1Loss()\n",
    "    loss_pix2pix = l1_loss(target, predicted)\n",
    "\n",
    "    # topological_loss\n",
    "    target_n = target.detach().cpu().clone().numpy()\n",
    "    predicted_n = predicted.detach().cpu().clone().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    topo_loss = []\n",
    "    \n",
    "\n",
    "    for i in range(len(target_n)):\n",
    "\n",
    "        cur_target = target_n[i]\n",
    "        cur_predicted = predicted_n[i]\n",
    "\n",
    "        target_t = eigen_centrality(cur_target)\n",
    "        real_topology = torch.tensor(target_t[0])\n",
    "        predicted_t = eigen_centrality(cur_predicted)\n",
    "        fake_topology = torch.tensor(predicted_t[0])\n",
    "        topo_loss.append(l1_loss(real_topology, fake_topology))\n",
    "\n",
    "    topo_loss = torch.sum(torch.stack(topo_loss))\n",
    "\n",
    "    pc_loss = pearson_coor(target, predicted).to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    G_loss = loss_pix2pix + (1 - pc_loss) + topo_loss\n",
    "\n",
    "    return G_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# put it back into a 2D symmetric array\n",
    "\n",
    "\n",
    "def topological_measures(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology = []\n",
    "\n",
    "\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_matrix(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "    # compute closeness centrality and transform the output to vector\n",
    "    cc = nx.closeness_centrality(U, distance=\"weight\")\n",
    "    closeness_centrality = np.array([cc[g] for g in U])\n",
    "    # compute betweeness centrality and transform the output to vector\n",
    "    # bc = nx.betweenness_centrality(U, weight='weight')\n",
    "    # bc = (nx.betweenness_centrality(U))\n",
    "    betweenness_centrality = np.array([cc[g] for g in U])\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "    topology.append(closeness_centrality)  # 0\n",
    "    topology.append(betweenness_centrality)  # 1\n",
    "    topology.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology\n",
    "# put it back into a 2D symmetric array\n",
    "\n",
    "def eigen_centrality(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology_eigen = []\n",
    "\n",
    "\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_array(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "\n",
    "    topology_eigen.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology_eigen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((2*N_LR_NODES, 16))\n",
    "adj = lr_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SheafAligner.__init__() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aligner \u001b[38;5;241m=\u001b[39m \u001b[43mSheafAligner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m aligned \u001b[38;5;241m=\u001b[39m aligner(X\u001b[38;5;241m.\u001b[39mto(DEVICE), adj\u001b[38;5;241m.\u001b[39mto(DEVICE)) \u001b[38;5;66;03m# should return (n_lr * d) * f matrix\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: SheafAligner.__init__() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "aligner = SheafAligner(2).to('cuda')\n",
    "aligned = aligner(X.to(DEVICE), adj.to(DEVICE)) # should return (n_lr * d) * f matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aligned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maligned\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aligned' is not defined"
     ]
    }
   ],
   "source": [
    "aligned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SheafGenerator(2).to('cuda')\n",
    "generated = generator(aligned, adj.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 268])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.randn((N_HR_NODES*2, 16))\n",
    "discriminator = SheafDiscriminator(2).to('cuda')\n",
    "dis_decision = discriminator(Y.to(DEVICE), generated.to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_data_tensor\n",
    "\n",
    "lr_train, lr_test, hr_train = load_data_tensor(\"dgl-icl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_X_dim1 = torch.load('model_autoencoder/encode_lr_1.pt')\n",
    "lr_X_dim2 = torch.load('model_autoencoder/encode_lr_2.pt')\n",
    "hr_X_dim1 = torch.load('model_autoencoder/encode_hr_1.pt')\n",
    "hr_X_dim2 = torch.load('model_autoencoder/encode_hr_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_X_dim1 = lr_X_dim1.detach()\n",
    "lr_X_dim2 = lr_X_dim2.detach()\n",
    "hr_X_dim1 = hr_X_dim1.detach()\n",
    "hr_X_dim2 = hr_X_dim2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_X_all = torch.empty((167, 320, 32))\n",
    "for i in range(len(lr_X_dim1)):\n",
    "    a, b = lr_X_dim1[i], lr_X_dim2[i]\n",
    "    lr_X_all[i] = torch.cat([a, b], dim=-1).view(-1, a.shape[-1])\n",
    "\n",
    "hr_X_all = torch.empty((167, 536, 32))\n",
    "for i in range(len(hr_X_dim1)):\n",
    "    a, b = hr_X_dim1[i], hr_X_dim2[i]\n",
    "    hr_X_all[i] = torch.cat([a, b], dim=-1).view(-1, a.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # number of dimensions in each node\n",
    "f = 32 # length of node encoding\n",
    "\n",
    "aligner = SheafAligner(d, f).to(DEVICE)\n",
    "generator = SheafGenerator(d, f).to(DEVICE)\n",
    "discriminator = SheafDiscriminator(d, f).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2445361"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in aligner.parameters()) + sum(p.numel() for p in generator.parameters()) + sum(p.numel() for p in discriminator.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner_optimizer = torch.optim.AdamW(aligner.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: align_loss = 813.017333984375, generator_loss = 2.558507073100601, discriminator = 0.6588972806930542\n",
      "sample 1: align_loss = 813.0, generator_loss = 1.4478594803673734, discriminator = 2.183009147644043\n",
      "sample 2: align_loss = 812.9721069335938, generator_loss = 1.7353280504417787, discriminator = 2.40781831741333\n",
      "sample 3: align_loss = 812.940185546875, generator_loss = 2.098929808726028, discriminator = 0.8243346214294434\n",
      "sample 4: align_loss = 812.9061279296875, generator_loss = 2.8762347905090886, discriminator = 0.7188798189163208\n",
      "sample 5: align_loss = 812.8638916015625, generator_loss = 1.9062400531189905, discriminator = 0.9488614797592163\n",
      "sample 6: align_loss = 812.8260498046875, generator_loss = 1.819497040923511, discriminator = 1.3266419172286987\n",
      "sample 7: align_loss = 812.7915649414062, generator_loss = 2.073112754254039, discriminator = 0.7897952198982239\n",
      "sample 8: align_loss = 812.7598876953125, generator_loss = 2.175923009782271, discriminator = 0.7228946089744568\n",
      "sample 9: align_loss = 812.734130859375, generator_loss = 1.9542621478240476, discriminator = 0.9037225246429443\n",
      "sample 10: align_loss = 812.694091796875, generator_loss = 2.025492338506016, discriminator = 0.8541136384010315\n",
      "sample 11: align_loss = 812.675048828125, generator_loss = 1.9348725217450822, discriminator = 1.0692216157913208\n",
      "sample 12: align_loss = 812.6397705078125, generator_loss = 2.581793837780667, discriminator = 0.7362640500068665\n",
      "sample 13: align_loss = 812.608642578125, generator_loss = 1.872658143064168, discriminator = 1.4595882892608643\n",
      "sample 14: align_loss = 812.5787353515625, generator_loss = 1.8012179711106213, discriminator = 1.373268961906433\n",
      "sample 15: align_loss = 812.5585327148438, generator_loss = 2.8635545504915862, discriminator = 0.7006757855415344\n",
      "sample 16: align_loss = 812.5362548828125, generator_loss = 1.6388949167583455, discriminator = 2.102205753326416\n",
      "sample 17: align_loss = 812.5103149414062, generator_loss = 1.7655010139462193, discriminator = 1.5723483562469482\n",
      "sample 18: align_loss = 812.4996337890625, generator_loss = 5.8007070994566945, discriminator = 1.9258168935775757\n",
      "sample 19: align_loss = 812.4694213867188, generator_loss = 1.8003962634115933, discriminator = 5.272132873535156\n",
      "sample 20: align_loss = 812.4479370117188, generator_loss = 1.7850347085846299, discriminator = 6.448932647705078\n",
      "sample 21: align_loss = 812.43310546875, generator_loss = 1.6635197681661082, discriminator = 2.7165565490722656\n",
      "sample 22: align_loss = 812.411865234375, generator_loss = 1.8465234628793767, discriminator = 2.442857265472412\n",
      "sample 23: align_loss = 812.3961181640625, generator_loss = 3.5754973201837896, discriminator = 1.1810177564620972\n",
      "sample 24: align_loss = 812.3775024414062, generator_loss = 2.8644199910106347, discriminator = 0.7478928565979004\n",
      "sample 25: align_loss = 812.3602905273438, generator_loss = 1.7467699045349196, discriminator = 3.2334728240966797\n",
      "sample 26: align_loss = 812.3425903320312, generator_loss = 1.6762934230480857, discriminator = 3.45725154876709\n",
      "sample 27: align_loss = 812.3282470703125, generator_loss = 1.6088467742135102, discriminator = 1.8438584804534912\n",
      "sample 28: align_loss = 812.3052978515625, generator_loss = 1.8308289034278233, discriminator = 3.4773619174957275\n",
      "sample 29: align_loss = 812.2943725585938, generator_loss = 4.2271006110411555, discriminator = 1.474233627319336\n",
      "sample 30: align_loss = 812.2806396484375, generator_loss = 1.7269595392553523, discriminator = 1.3497016429901123\n",
      "sample 31: align_loss = 812.2646484375, generator_loss = 1.7696464211862482, discriminator = 4.375156402587891\n",
      "sample 32: align_loss = 812.2501220703125, generator_loss = 1.7872389851605366, discriminator = 3.3298585414886475\n",
      "sample 33: align_loss = 812.23828125, generator_loss = 2.3884279448925567, discriminator = 0.6436636447906494\n",
      "sample 34: align_loss = 812.2261962890625, generator_loss = 1.8913639005793321, discriminator = 1.0746477842330933\n",
      "sample 35: align_loss = 812.212890625, generator_loss = 2.5775990383986986, discriminator = 0.6227842569351196\n",
      "sample 36: align_loss = 812.1973876953125, generator_loss = 1.7760446670669283, discriminator = 2.5768539905548096\n",
      "sample 37: align_loss = 812.2030639648438, generator_loss = 2.3974446659681217, discriminator = 0.7010577917098999\n",
      "sample 38: align_loss = 812.1801147460938, generator_loss = 1.8732671526847307, discriminator = 0.900872528553009\n",
      "sample 39: align_loss = 812.178955078125, generator_loss = 1.8616800777766114, discriminator = 1.4776947498321533\n",
      "sample 40: align_loss = 812.158935546875, generator_loss = 2.946218012517315, discriminator = 0.6179513931274414\n",
      "sample 41: align_loss = 812.1473388671875, generator_loss = 1.8048328746284616, discriminator = 1.3134979009628296\n",
      "sample 42: align_loss = 812.139404296875, generator_loss = 1.6534471140154559, discriminator = 1.6522215604782104\n",
      "sample 43: align_loss = 812.1332397460938, generator_loss = 2.4778608367958803, discriminator = 0.8064757585525513\n",
      "sample 44: align_loss = 812.1197509765625, generator_loss = 2.0247421119493607, discriminator = 0.8014477491378784\n",
      "sample 45: align_loss = 812.11279296875, generator_loss = 1.986022867348815, discriminator = 1.0509958267211914\n",
      "sample 46: align_loss = 812.1043701171875, generator_loss = 2.160297809483788, discriminator = 0.6952235698699951\n",
      "sample 47: align_loss = 812.0969848632812, generator_loss = 2.235922119899371, discriminator = 0.7001577019691467\n",
      "sample 48: align_loss = 812.0875244140625, generator_loss = 1.785751679705662, discriminator = 2.2412407398223877\n",
      "sample 49: align_loss = 812.0855102539062, generator_loss = 2.631256313751077, discriminator = 0.5026572942733765\n",
      "sample 50: align_loss = 812.0810546875, generator_loss = 1.7858327485117815, discriminator = 1.8912931680679321\n",
      "sample 51: align_loss = 812.0699462890625, generator_loss = 1.96444866771249, discriminator = 1.162354588508606\n",
      "sample 52: align_loss = 812.0609741210938, generator_loss = 1.965685834981879, discriminator = 1.1383079290390015\n",
      "sample 53: align_loss = 812.055419921875, generator_loss = 2.740668884318336, discriminator = 0.800570011138916\n",
      "sample 54: align_loss = 812.04541015625, generator_loss = 1.790577569898277, discriminator = 2.7661657333374023\n",
      "sample 55: align_loss = 812.0386962890625, generator_loss = 1.7892612136983272, discriminator = 2.5216073989868164\n",
      "sample 56: align_loss = 812.0297241210938, generator_loss = 2.1095623913845056, discriminator = 0.828494668006897\n",
      "sample 57: align_loss = 812.0267333984375, generator_loss = 2.918625371029574, discriminator = 0.973640501499176\n",
      "sample 58: align_loss = 812.021240234375, generator_loss = 1.7352222949790872, discriminator = 2.7224950790405273\n",
      "sample 59: align_loss = 812.0128173828125, generator_loss = 1.753772375584385, discriminator = 2.677096128463745\n",
      "sample 60: align_loss = 812.009033203125, generator_loss = 1.768340534789105, discriminator = 1.4654649496078491\n",
      "sample 61: align_loss = 812.00146484375, generator_loss = 2.5021924972763028, discriminator = 0.731298565864563\n",
      "sample 62: align_loss = 811.9953002929688, generator_loss = 2.701925492239302, discriminator = 0.4590747356414795\n",
      "sample 63: align_loss = 811.9989013671875, generator_loss = 1.8076652938383604, discriminator = 1.514262318611145\n",
      "sample 64: align_loss = 811.9906616210938, generator_loss = 1.9372800295769346, discriminator = 1.161821961402893\n",
      "sample 65: align_loss = 811.9886474609375, generator_loss = 2.250963618769003, discriminator = 0.6197980642318726\n",
      "sample 66: align_loss = 811.9813232421875, generator_loss = 2.047527297635585, discriminator = 0.9578135013580322\n",
      "sample 67: align_loss = 811.9765625, generator_loss = 2.148036223749065, discriminator = 0.7519677877426147\n",
      "sample 68: align_loss = 811.9730224609375, generator_loss = 1.9607447852147064, discriminator = 0.948692262172699\n",
      "sample 69: align_loss = 811.9725341796875, generator_loss = 2.3850082567546362, discriminator = 0.7044529914855957\n",
      "sample 70: align_loss = 811.9611206054688, generator_loss = 1.8350811952190567, discriminator = 1.3961032629013062\n",
      "sample 71: align_loss = 811.9599609375, generator_loss = 1.7354570288090687, discriminator = 0.8292601704597473\n",
      "sample 72: align_loss = 811.9630737304688, generator_loss = 1.984660505882664, discriminator = 0.9227162599563599\n",
      "sample 73: align_loss = 811.9515991210938, generator_loss = 2.7902626797365544, discriminator = 0.7083765268325806\n",
      "sample 74: align_loss = 811.9517822265625, generator_loss = 1.8843175216180736, discriminator = 1.1685410737991333\n",
      "sample 75: align_loss = 811.9464111328125, generator_loss = 1.7475858315321129, discriminator = 1.944464921951294\n",
      "sample 76: align_loss = 811.9459838867188, generator_loss = 1.7801471089218115, discriminator = 1.4768248796463013\n",
      "sample 77: align_loss = 811.9395751953125, generator_loss = 6.289228246185815, discriminator = 1.473775863647461\n",
      "sample 78: align_loss = 811.935546875, generator_loss = 1.7278932593508538, discriminator = 2.4768075942993164\n",
      "sample 79: align_loss = 811.9330444335938, generator_loss = 1.7691538426902431, discriminator = 2.8239188194274902\n",
      "sample 80: align_loss = 811.9290161132812, generator_loss = 1.742015194289119, discriminator = 2.174854040145874\n",
      "sample 81: align_loss = 811.9255981445312, generator_loss = 2.4074926672866193, discriminator = 0.5643836259841919\n",
      "sample 82: align_loss = 811.923095703125, generator_loss = 2.7942170957537513, discriminator = 0.5316566824913025\n",
      "sample 83: align_loss = 811.9201049804688, generator_loss = 1.7566062528772182, discriminator = 1.9344578981399536\n",
      "sample 84: align_loss = 811.91845703125, generator_loss = 1.8014128966713099, discriminator = 2.110321521759033\n",
      "sample 85: align_loss = 811.91650390625, generator_loss = 2.0876358706243376, discriminator = 0.7509809732437134\n",
      "sample 86: align_loss = 811.912109375, generator_loss = 3.632348544411003, discriminator = 1.1440651416778564\n",
      "sample 87: align_loss = 811.91162109375, generator_loss = 1.7772377349655393, discriminator = 3.001614570617676\n",
      "sample 88: align_loss = 811.908203125, generator_loss = 1.8262853563853514, discriminator = 4.174505710601807\n",
      "sample 89: align_loss = 811.903076171875, generator_loss = 1.839651268419965, discriminator = 3.0101051330566406\n",
      "sample 90: align_loss = 811.9031982421875, generator_loss = 2.1985553355810765, discriminator = 0.8006767630577087\n",
      "sample 91: align_loss = 811.9027709960938, generator_loss = 3.0359421166396707, discriminator = 0.6539156436920166\n",
      "sample 92: align_loss = 811.8994140625, generator_loss = 1.812021293611132, discriminator = 1.4426690340042114\n",
      "sample 93: align_loss = 811.8963623046875, generator_loss = 1.7117731701972336, discriminator = 2.056187868118286\n",
      "sample 94: align_loss = 811.8941040039062, generator_loss = 2.227414411725809, discriminator = 0.6772423982620239\n",
      "sample 95: align_loss = 811.890625, generator_loss = 2.04379548293457, discriminator = 0.783159077167511\n",
      "sample 96: align_loss = 811.8929443359375, generator_loss = 2.0528220362715808, discriminator = 0.8413549661636353\n",
      "sample 97: align_loss = 811.88916015625, generator_loss = 2.2481841967483915, discriminator = 0.6074244379997253\n",
      "sample 98: align_loss = 811.8876953125, generator_loss = 2.0136843308183705, discriminator = 0.8361753225326538\n",
      "sample 99: align_loss = 811.8839111328125, generator_loss = 1.8614704645554414, discriminator = 1.4831857681274414\n",
      "sample 100: align_loss = 811.8837890625, generator_loss = 2.7330559177799802, discriminator = 0.7266807556152344\n",
      "sample 101: align_loss = 811.8802490234375, generator_loss = 1.7377805590978295, discriminator = 1.605992078781128\n",
      "sample 102: align_loss = 811.8778076171875, generator_loss = 1.749811317753136, discriminator = 1.4569600820541382\n",
      "sample 103: align_loss = 811.8753662109375, generator_loss = 2.5822517416553916, discriminator = 0.6199404001235962\n",
      "sample 104: align_loss = 811.8768310546875, generator_loss = 1.7863791979372918, discriminator = 1.365325689315796\n",
      "sample 105: align_loss = 811.8731079101562, generator_loss = 1.9565595596707952, discriminator = 1.102793574333191\n",
      "sample 106: align_loss = 811.8722534179688, generator_loss = 2.269371031714182, discriminator = 1.0696704387664795\n",
      "sample 107: align_loss = 811.86962890625, generator_loss = 2.114001842260376, discriminator = 0.9225229024887085\n",
      "sample 108: align_loss = 811.8689575195312, generator_loss = 2.060840008938569, discriminator = 0.8495461940765381\n",
      "sample 109: align_loss = 811.866943359375, generator_loss = 1.9521923157686987, discriminator = 0.9590030908584595\n",
      "sample 110: align_loss = 811.8646850585938, generator_loss = 1.9950861030876514, discriminator = 0.8363121747970581\n",
      "sample 111: align_loss = 811.86376953125, generator_loss = 2.1251661564447692, discriminator = 0.7989883422851562\n",
      "sample 112: align_loss = 811.86376953125, generator_loss = 2.3383439772414776, discriminator = 0.49198782444000244\n",
      "sample 113: align_loss = 811.86083984375, generator_loss = 1.9444400867464824, discriminator = 1.0631023645401\n",
      "sample 114: align_loss = 811.861083984375, generator_loss = 1.876660982981413, discriminator = 1.0091360807418823\n",
      "sample 115: align_loss = 811.8580322265625, generator_loss = 2.1211136005317464, discriminator = 0.8211040496826172\n",
      "sample 116: align_loss = 811.855712890625, generator_loss = 1.9424770464045984, discriminator = 0.8489792346954346\n",
      "sample 117: align_loss = 811.8543701171875, generator_loss = 1.836045151501619, discriminator = 1.0573543310165405\n",
      "sample 118: align_loss = 811.8543701171875, generator_loss = 2.0923845051275687, discriminator = 0.8722200989723206\n",
      "sample 119: align_loss = 811.8556518554688, generator_loss = 1.7964387916034301, discriminator = 0.9550884962081909\n",
      "sample 120: align_loss = 811.8524169921875, generator_loss = 2.0681411584238316, discriminator = 0.757423460483551\n",
      "sample 121: align_loss = 811.8522338867188, generator_loss = 2.731328635197317, discriminator = 1.0104312896728516\n",
      "sample 122: align_loss = 811.8511962890625, generator_loss = 1.74804253267001, discriminator = 1.4611425399780273\n",
      "sample 123: align_loss = 811.84716796875, generator_loss = 1.840128219050458, discriminator = 1.5038962364196777\n",
      "sample 124: align_loss = 811.8474731445312, generator_loss = 2.1121413059266523, discriminator = 0.7191067934036255\n",
      "sample 125: align_loss = 811.8483276367188, generator_loss = 2.2915529661600003, discriminator = 0.642124354839325\n",
      "sample 126: align_loss = 811.8433837890625, generator_loss = 1.9374491377311518, discriminator = 1.0940394401550293\n",
      "sample 127: align_loss = 811.8408203125, generator_loss = 2.010615019426261, discriminator = 0.7905334234237671\n",
      "sample 128: align_loss = 811.8434448242188, generator_loss = 1.7942941951689337, discriminator = 1.4468746185302734\n",
      "sample 129: align_loss = 811.8429565429688, generator_loss = 1.8889661896472671, discriminator = 1.1052567958831787\n",
      "sample 130: align_loss = 811.8402099609375, generator_loss = 2.9333297593745096, discriminator = 0.5952315330505371\n",
      "sample 131: align_loss = 811.837646484375, generator_loss = 1.9434729406006312, discriminator = 0.819456934928894\n",
      "sample 132: align_loss = 811.8377075195312, generator_loss = 1.791380053223108, discriminator = 1.921927809715271\n",
      "sample 133: align_loss = 811.837158203125, generator_loss = 2.1343280369203077, discriminator = 0.7187281250953674\n",
      "sample 134: align_loss = 811.8363037109375, generator_loss = 2.1995514406458074, discriminator = 0.6814575791358948\n",
      "sample 135: align_loss = 811.8341674804688, generator_loss = 1.8253756257579943, discriminator = 1.3112969398498535\n",
      "sample 136: align_loss = 811.8357543945312, generator_loss = 2.6342629965534536, discriminator = 0.668140172958374\n",
      "sample 137: align_loss = 811.833984375, generator_loss = 2.0136701180166634, discriminator = 0.9372556805610657\n",
      "sample 138: align_loss = 811.83349609375, generator_loss = 2.1465569309401964, discriminator = 0.7634159326553345\n",
      "sample 139: align_loss = 811.8314208984375, generator_loss = 1.7376850556215226, discriminator = 3.030911445617676\n",
      "sample 140: align_loss = 811.8312377929688, generator_loss = 1.7340817951980663, discriminator = 1.9799330234527588\n",
      "sample 141: align_loss = 811.8275146484375, generator_loss = 2.472402484121097, discriminator = 0.6177421808242798\n",
      "sample 142: align_loss = 811.8295288085938, generator_loss = 2.036034378067022, discriminator = 0.8522976040840149\n",
      "sample 143: align_loss = 811.82763671875, generator_loss = 2.0721575184163914, discriminator = 0.8425900936126709\n",
      "sample 144: align_loss = 811.8271484375, generator_loss = 1.9923241056646612, discriminator = 0.9848690032958984\n",
      "sample 145: align_loss = 811.8269653320312, generator_loss = 2.101374307826026, discriminator = 0.866280198097229\n",
      "sample 146: align_loss = 811.826171875, generator_loss = 1.9012581739586496, discriminator = 0.9488269686698914\n",
      "sample 147: align_loss = 811.8243408203125, generator_loss = 2.149865618227712, discriminator = 0.7884609699249268\n",
      "sample 148: align_loss = 811.8246459960938, generator_loss = 2.0190742302277407, discriminator = 0.9647864103317261\n",
      "sample 149: align_loss = 811.824462890625, generator_loss = 2.0822428358472784, discriminator = 0.9031470417976379\n",
      "sample 150: align_loss = 811.822998046875, generator_loss = 2.1842962884222334, discriminator = 0.7916520237922668\n",
      "sample 151: align_loss = 811.8248291015625, generator_loss = 1.970754982622503, discriminator = 1.277402639389038\n",
      "sample 152: align_loss = 811.8209228515625, generator_loss = 1.80743162378444, discriminator = 1.749255657196045\n",
      "sample 153: align_loss = 811.8199462890625, generator_loss = 2.0004037141204076, discriminator = 0.8763073086738586\n",
      "sample 154: align_loss = 811.822509765625, generator_loss = 1.9184357573324233, discriminator = 0.8724474906921387\n",
      "sample 155: align_loss = 811.8172607421875, generator_loss = 2.6265595136420363, discriminator = 0.6273389458656311\n",
      "sample 156: align_loss = 811.818603515625, generator_loss = 1.7834696094013127, discriminator = 1.6209672689437866\n",
      "sample 157: align_loss = 811.8181762695312, generator_loss = 1.9652423704587492, discriminator = 0.8781682848930359\n",
      "sample 158: align_loss = 811.8153076171875, generator_loss = 2.495007754844704, discriminator = 0.6532140970230103\n",
      "sample 159: align_loss = 811.8165893554688, generator_loss = 1.8530884044979312, discriminator = 1.304038166999817\n",
      "sample 160: align_loss = 811.8156127929688, generator_loss = 1.8256944095835397, discriminator = 1.323848009109497\n",
      "sample 161: align_loss = 811.8150634765625, generator_loss = 2.6885416547364667, discriminator = 0.6309547424316406\n",
      "sample 162: align_loss = 811.8131103515625, generator_loss = 1.7390359722828284, discriminator = 1.7790499925613403\n",
      "sample 163: align_loss = 811.8133544921875, generator_loss = 2.346434409168959, discriminator = 0.5340845584869385\n",
      "sample 164: align_loss = 811.8125, generator_loss = 1.9224263845542862, discriminator = 1.0328036546707153\n",
      "sample 165: align_loss = 811.81298828125, generator_loss = 2.973229001970696, discriminator = 0.7695189118385315\n",
      "sample 166: align_loss = 811.8126831054688, generator_loss = 1.794626806035955, discriminator = 3.2259132862091064\n"
     ]
    }
   ],
   "source": [
    "aligner.train()\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "    \n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for i, sample in enumerate(zip(lr_X_all, lr_train, hr_X_all, hr_train)):\n",
    "        X_lr, adj_lr, X_hr, adj_hr = sample\n",
    "        aligner_optimizer.zero_grad()\n",
    "        generator_optimizer.zero_grad()\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        aligned_X_lr, aligned_adj_lr = aligner(X_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        hr_mean = torch.mean(X_hr)\n",
    "        hr_std = torch.std(X_hr)\n",
    "\n",
    "        adj_hr_sampled = torch.normal(hr_mean, hr_std, size=(N_LR_NODES, N_LR_NODES)).to(DEVICE)\n",
    "        # hr_X_sampled = torch.Tensor(MatrixVectorizer().anti_vectorize(hr_X_sampled, N_HR_NODES))\n",
    "\n",
    "\n",
    "        alignment_loss = torch.abs(F.kl_div(F.softmax(adj_hr_sampled, dim=-1), F.softmax(aligned_adj_lr, dim=-1), None, None, 'sum'))\n",
    "\n",
    "\n",
    "\n",
    "        # generate hr adjacency\n",
    "        generated_adj_hr = generator(aligned_X_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # calculate losses for generator\n",
    "\n",
    "        #### NOTE TEMPORARY MEASURE BECAUSE THEY TAKE IN (BATCHSIZE, xx, xx) shape ####\n",
    "        temp_adj_hr = adj_hr.reshape(1, *adj_hr.shape)\n",
    "        temp_generated_adj_hr = generated_adj_hr.reshape(1, *generated_adj_hr.shape)\n",
    "        ##########################################################\n",
    "\n",
    "        g_topology_loss = GT_loss(temp_adj_hr.to(DEVICE), temp_generated_adj_hr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_real = discriminator(X_hr.to(DEVICE), adj_hr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_fake = discriminator(X_hr.to(DEVICE), generated_adj_hr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        g_adversarial_loss = adversarial_loss(d_fake, (torch.ones_like(d_fake, requires_grad=False)))\n",
    "        g_loss = g_adversarial_loss + g_topology_loss\n",
    "\n",
    "\n",
    "\n",
    "        d_real_loss = adversarial_loss(d_real, torch.ones_like(d_real, requires_grad=False))\n",
    "        torch.cuda.empty_cache()\n",
    "        d_fake_loss = adversarial_loss(d_fake, torch.zeros_like(d_fake, requires_grad=False))\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        alignment_loss.backward(retain_graph=True)\n",
    "        aligner_optimizer.step()\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        print(f'sample {i}: align_loss = {alignment_loss}, generator_loss = {g_loss}, discriminator = {d_loss}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
