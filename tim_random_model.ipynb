{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh, Dropout, Upsample\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.distributions import normal, kl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from MatrixVectorizer import MatrixVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "N_SUBJECTS = 167\n",
    "\n",
    "N_LR_NODES = 160\n",
    "\n",
    "N_HR_NODES = 268\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "N_LR_NODES_F = int(N_LR_NODES * (N_LR_NODES-1) / 2)\n",
    "N_HR_NODES_F = int(N_HR_NODES * (N_HR_NODES-1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafConvLayer(nn.Module):\n",
    "    def __init__(self, n_nodes, d, f_in, f_out=None):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.n_nodes = n_nodes\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        # random init weight matrices\n",
    "        if f_out is None:\n",
    "            f_out = f_in \n",
    "        self.weight1 = nn.Parameter(torch.randn((d, d), device=DEVICE))\n",
    "        self.weight2 = nn.Parameter(torch.randn((f_in, f_out), device=DEVICE))\n",
    "        self.edge_weights = nn.Parameter(torch.randn((n_nodes, n_nodes, d, 2*d), device=DEVICE))\n",
    "\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        kron_prod = torch.kron(torch.eye(self.n_nodes).to(DEVICE), self.weight1)\n",
    "        L = self.sheaf_laplacian(X, adj)\n",
    "        if self.f_out is None:\n",
    "            return X - F.elu(L @ kron_prod @ X @ self.weight2), L\n",
    "        else:\n",
    "            return F.elu(L @ kron_prod @ X @ self.weight2), L\n",
    "\n",
    "\n",
    "    def sheaf_laplacian(self, X, adj, epsilon=1e-6):\n",
    "        X_reshaped = X.reshape(self.n_nodes, self.d, -1)\n",
    "        idx_pairs = torch.cartesian_prod(torch.arange(self.n_nodes), torch.arange(self.n_nodes))\n",
    "        all_stacked_features = X_reshaped[idx_pairs].reshape(self.n_nodes, self.n_nodes, 2*self.d, -1).to(DEVICE)\n",
    "        lin_trans = F.elu(torch.matmul(self.edge_weights, all_stacked_features))\n",
    "        inner_transpose = torch.transpose(lin_trans, -1, -2)\n",
    "        L_v = -1 * torch.matmul(lin_trans, torch.transpose(inner_transpose, 0, 1))\n",
    "        # row_cond = torch.isclose(torch.sum(adj, dim=1), torch.zeros_like(torch.sum(adj, dim=1)))\n",
    "        # col_cond = torch.isclose(torch.sum(adj, dim=0), torch.zeros_like(torch.sum(adj, dim=0)))\n",
    "        adj_row_weights = adj / (torch.sum(adj, dim=1)[:, None] + epsilon)\n",
    "        adj_col_weights = adj / (torch.sum(adj, dim=0)[:, None] + epsilon)\n",
    "        # adj_col_weights = torch.where(col_cond[None, :], 0., adj / torch.sum(adj, dim=0)[None, :])\n",
    "        adj_weights = torch.maximum(adj_row_weights * adj_col_weights, torch.zeros_like(adj_row_weights))\n",
    "\n",
    "        adj_diag_weights = adj_row_weights ** 2\n",
    "        diag_blocks = torch.sum(adj_diag_weights[:, :, None, None] * torch.matmul(lin_trans, inner_transpose), dim=1)\n",
    "        L_v[range(self.n_nodes), range(self.n_nodes)] = diag_blocks\n",
    "        return L_v.reshape(-1, self.n_nodes * self.d)\n",
    "        ### NOTE IGNORE MATRIX NORMALISATION FOR NOW #####\n",
    "        # inv_root_diag_blocks = torch.pow(diag_blocks+epsilon, -1/2)\n",
    "        # normalise_mat = torch.block_diag(*inv_root_diag_blocks)\n",
    "\n",
    "        # return normalise_mat @ L_v.view(-1, self.n_nodes * self.d) @ normalise_mat\n",
    "        ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyDimChanger(nn.Module):\n",
    "    def __init__(self, new_n, old_n, old_f, d):\n",
    "        super().__init__()\n",
    "        self.new_n = new_n\n",
    "        self.old_n = old_n\n",
    "        self.d = d\n",
    "        self.sheafconv = SheafConvLayer(old_n, d, old_f, new_n)\n",
    "        self.layernorm = nn.LayerNorm([d, old_n]).to(DEVICE)\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "\n",
    "        adj = adj - torch.diag_embed(torch.diagonal(adj, 0)).to(DEVICE) + torch.eye(adj.shape[0]).to(DEVICE)  # add self connections\n",
    "        x, L = self.sheafconv(X, adj)\n",
    "\n",
    "        x = x.reshape(self.old_n, self.d, self.new_n)\n",
    "        x = torch.transpose(x, 0, -1)\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        x_mean = x.mean(dim=-1)\n",
    "\n",
    "        L_mean = L.reshape(self.old_n, self.old_n, self.d, self.d).max(dim=0)[0].mean(dim=0) # aggregate by eigenvalues of each n by n mat?\n",
    "        adj_new = torch.matmul(x_mean, L_mean)\n",
    "        adj_new = torch.matmul(adj_new, x_mean.T)\n",
    "        adj_new_T = torch.t(adj_new)\n",
    "        adj_new = F.tanh(F.relu(((adj_new + adj_new_T) / 2))) # becomes a new f by new f adj1\n",
    "\n",
    "        return x.reshape(self.new_n*self.d, -1), adj_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyChangerUp(nn.Module):\n",
    "\n",
    "    def __init__(self, d, f_in):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "\n",
    "        self.adjdim_changer1 = AdjacencyDimChanger(200, N_LR_NODES, f_in, d)\n",
    "        self.adjdim_changer2 = AdjacencyDimChanger(220, 200, N_LR_NODES, d)\n",
    "        self.adjdim_changer3 = AdjacencyDimChanger(N_HR_NODES, 220, 200, d)\n",
    "\n",
    "        \n",
    "    def forward(self, X, adj):\n",
    "        x1, adj1 = self.adjdim_changer1(X, adj)\n",
    "        x2, adj2 = self.adjdim_changer2(x1, adj1)\n",
    "        x3, adj3 = self.adjdim_changer3(x2, adj2)\n",
    "        return [adj, adj1, adj2, adj3]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyChangerDown(nn.Module):\n",
    "\n",
    "    def __init__(self, d, f_in):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "\n",
    "        self.adjdim_changer1 = AdjacencyDimChanger(220, N_HR_NODES, f_in, d).to(DEVICE)\n",
    "        self.adjdim_changer2  = AdjacencyDimChanger(200, 220, N_HR_NODES, d).to(DEVICE)\n",
    "        self.adjdim_changer3 = AdjacencyDimChanger(N_LR_NODES, 200, 220, d).to(DEVICE)\n",
    "\n",
    "        \n",
    "    def forward(self, X, adj):\n",
    "        x1, adj1 = self.adjdim_changer1(X, adj)\n",
    "        x2, adj2 = self.adjdim_changer2(x1, adj1)\n",
    "        x3, adj3 = self.adjdim_changer3(x2, adj2)\n",
    "        return [adj, adj1, adj2, adj3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def eigen_centrality(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology_eigen = []\n",
    "\n",
    "    G = nx.from_numpy_array(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph frL2\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    \n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "\n",
    "    topology_eigen.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology_eigen\n",
    "\n",
    "def pearson_coor(input, target, epsilon=1e-7):\n",
    "    vx = input - torch.mean(input, dim=(1, 2))[:, None, None]\n",
    "    vy = target - torch.mean(target, dim=(1, 2))[:, None, None]\n",
    "    cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)+epsilon) * torch.sqrt(torch.sum(vy ** 2)+epsilon)+epsilon)\n",
    "    return cost\n",
    "\n",
    "def GT_loss(target, predicted):\n",
    "\n",
    "    # l1_loss\n",
    "    l1_loss = torch.nn.L1Loss()\n",
    "    # loss_pix2pix = l1_loss(target, predicted)\n",
    "\n",
    "    # topological_loss\n",
    "    target_n = target.detach().cpu().clone().numpy()\n",
    "    predicted_n = predicted.detach().cpu().clone().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    topo_loss = []\n",
    "    \n",
    "\n",
    "    for i in range(len(target_n)):\n",
    "\n",
    "        cur_target = target_n[i]\n",
    "        cur_predicted = predicted_n[i]\n",
    "\n",
    "        target_t = eigen_centrality(cur_target)\n",
    "        real_topology = torch.tensor(target_t[0])\n",
    "        predicted_t = eigen_centrality(cur_predicted)\n",
    "        fake_topology = torch.tensor(predicted_t[0])\n",
    "        topo_loss.append(l1_loss(real_topology, fake_topology))\n",
    "\n",
    "    topo_loss = torch.sum(torch.stack(topo_loss))\n",
    "\n",
    "    pc_loss = pearson_coor(target, predicted).to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # G_loss = loss_pix2pix + (1 - pc_loss) + topo_loss\n",
    "    G_loss = (1 - pc_loss) + topo_loss\n",
    "\n",
    "\n",
    "    return G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calc(adj_ls, opp_adj_ls):\n",
    "    total_loss = torch.Tensor([0]).to(DEVICE)\n",
    "    mse_loss_fn = torch.nn.MSELoss()\n",
    "    for i, (adj, opp_adj) in enumerate(zip(adj_ls[::-1], opp_adj_ls)):\n",
    "\n",
    "        ### NOTE TEMPORARY MEASURE BECAUSE THEY TAKE IN (BATCHSIZE, xx, xx) shape ####\n",
    "        temp_adj = adj.reshape(1, *adj.shape)\n",
    "        temp_opp_adj = opp_adj.reshape(1, *opp_adj.shape)\n",
    "        ##########################################################\n",
    "        gt_loss = GT_loss(temp_adj, temp_opp_adj) / len(adj_ls)\n",
    "        mse_loss = torch.pow(mse_loss_fn(adj, opp_adj), 1/(i+1)) \n",
    "        total_loss = total_loss + mse_loss + gt_loss.to(DEVICE)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_data_tensor\n",
    "\n",
    "lr_train, lr_test, hr_train = load_data_tensor(\"dgl-icl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_X_dim1 = torch.load('model_autoencoder/encode_lr_1.pt')\n",
    "lr_X_dim2 = torch.load('model_autoencoder/encode_lr_2.pt')\n",
    "hr_X_dim1 = torch.load('model_autoencoder/encode_hr_1.pt')\n",
    "hr_X_dim2 = torch.load('model_autoencoder/encode_hr_2.pt')\n",
    "\n",
    "\n",
    "lr_X_all = torch.empty((167, 320, 32))\n",
    "for i in range(len(lr_X_dim1)):\n",
    "    a, b = lr_X_dim1[i], lr_X_dim2[i]\n",
    "    lr_X_all[i] = torch.cat([a, b], dim=-1).view(-1, a.shape[-1])\n",
    "\n",
    "hr_X_all = torch.empty((167, 536, 32))\n",
    "for i in range(len(hr_X_dim1)):\n",
    "    a, b = hr_X_dim1[i], hr_X_dim2[i]\n",
    "    hr_X_all[i] = torch.cat([a, b], dim=-1).view(-1, a.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2389928"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = DataLoader(list(zip(lr_X_all, lr_train, hr_X_all, hr_train)), shuffle=True, batch_size=8)\n",
    "\n",
    "\n",
    "up_changer = AdjacencyChangerUp(d=2,f_in=32).to(DEVICE)\n",
    "down_changer = AdjacencyChangerDown(d=2,f_in=32).to(DEVICE)\n",
    "\n",
    "up_optimizer = torch.optim.AdamW(up_changer.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "down_optimizer = torch.optim.AdamW(down_changer.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "\n",
    "sum(p.numel() for model in [up_changer, down_changer] for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, up_changer, down_changer, trainloader, up_optimizer, down_optimizer):\n",
    "\n",
    "    up_changer.train()\n",
    "    down_changer.train()\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            up_losses = []\n",
    "            down_losses = []\n",
    "\n",
    "            for X_lr, adj_lr, X_hr, adj_hr in tqdm(trainloader):\n",
    "\n",
    "                freeze_model(up_changer)\n",
    "                unfreeze_model(down_changer)\n",
    "            \n",
    "                down_optimizer.zero_grad()\n",
    "                up_optimizer.zero_grad()\n",
    "\n",
    "                down_batch_loss = []\n",
    "\n",
    "                for i in range(len(X_lr)):\n",
    "\n",
    "                    up_adj_ls = up_changer(X_lr[i].to(DEVICE), adj_lr[i].to(DEVICE))\n",
    "                    torch.cuda.empty_cache()\n",
    "                    down_adj_ls = down_changer(X_hr[i].to(DEVICE), adj_hr[i].to(DEVICE))\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    down_batch_loss.append(loss_calc(down_adj_ls, up_adj_ls))\n",
    "\n",
    "                down_loss = torch.mean(torch.stack(down_batch_loss))\n",
    "                down_loss.backward()\n",
    "                down_optimizer.step()\n",
    "\n",
    "                down_losses.append(down_loss.detach().item())\n",
    "                del down_loss\n",
    "                del down_batch_loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                unfreeze_model(up_changer)\n",
    "                freeze_model(down_changer)\n",
    "            \n",
    "                down_optimizer.zero_grad()\n",
    "                up_optimizer.zero_grad()\n",
    "\n",
    "                up_batch_loss = []\n",
    "\n",
    "\n",
    "                for i in range(len(X_lr)):\n",
    "\n",
    "                    up_adj_ls = up_changer(X_lr[i].to(DEVICE), adj_lr[i].to(DEVICE))\n",
    "                    torch.cuda.empty_cache()\n",
    "                    down_adj_ls = down_changer(X_hr[i].to(DEVICE), adj_hr[i].to(DEVICE))\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                    up_batch_loss.append(loss_calc(up_adj_ls, down_adj_ls))\n",
    "\n",
    "                up_loss = torch.mean(torch.stack(up_batch_loss))\n",
    "                up_loss.backward()\n",
    "                up_optimizer.step()\n",
    "\n",
    "                up_losses.append(up_loss.detach().item())\n",
    "                del up_loss\n",
    "                del up_batch_loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "            epoch_up_loss = np.mean(up_losses)\n",
    "            epoch_down_loss = np.mean(down_losses)\n",
    "\n",
    "            print(f'epoch {epoch}: down loss = {epoch_down_loss}, up loss = {epoch_up_loss}')\n",
    "\n",
    "        return up_changer, down_changer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7/21 [05:51<11:35, 49.71s/it]"
     ]
    }
   ],
   "source": [
    "up_changer, down_changer = train(20, up_changer, down_changer, trainloader, up_optimizer, down_optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
